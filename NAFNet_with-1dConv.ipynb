{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 주석으로 설명 달아놓음\n",
    "# with 1d conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 관리\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# 이미지 처리\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# For Custom Dataset\n",
    "import torchvision.transforms.functional as TVF\n",
    "\n",
    "# Network\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# For Custom Network\n",
    "import math as m\n",
    "\n",
    "# Utils\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LayerNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormFunction(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight, bias, eps):\n",
    "        ctx.eps = eps\n",
    "        N, C, H, W = x.size()\n",
    "        mu = x.mean(1, keepdim=True)\n",
    "        var = (x - mu).pow(2).mean(1, keepdim=True)\n",
    "        y = (x - mu) / (var + eps).sqrt()\n",
    "        ctx.save_for_backward(y, var, weight)\n",
    "        y = weight.view(1, C, 1, 1) * y + bias.view(1, C, 1, 1)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        eps = ctx.eps\n",
    "\n",
    "        N, C, H, W = grad_output.size()\n",
    "        y, var, weight = ctx.saved_tensors\n",
    "        g = grad_output * weight.view(1, C, 1, 1)\n",
    "        mean_g = g.mean(dim=1, keepdim=True)\n",
    "\n",
    "        mean_gy = (g * y).mean(dim=1, keepdim=True)\n",
    "        gx = 1. / torch.sqrt(var + eps) * (g - y * mean_gy - mean_g)\n",
    "        return gx, (grad_output * y).sum(dim=3).sum(dim=2).sum(dim=0), grad_output.sum(dim=3).sum(dim=2).sum(\n",
    "            dim=0), None\n",
    "\n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, channels, eps=1e-6):\n",
    "        super(LayerNorm2d, self).__init__()\n",
    "        self.register_parameter('weight', nn.Parameter(torch.ones(channels)))\n",
    "        self.register_parameter('bias', nn.Parameter(torch.zeros(channels)))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return LayerNormFunction.apply(x, self.weight, self.bias, self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAFNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel 방향으로 쪼개서 Element-wise mul 진행\n",
    "class SimpleGate(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x1, x2 = x.chunk(2, dim=1)  # (batch, channel, row, col)\n",
    "        return x1 * x2\n",
    "\n",
    "class NAFBlock(nn.Module):\n",
    "    def __init__(self, c, DW_Expand=2, FFN_Expand=2, drop_out_rate=0.):\n",
    "        super(NAFBlock, self).__init__()\n",
    "        dw_channel = c * DW_Expand  # Embedding 과정과, Non-linear 과정에서 정보유실을 방지하기위해 Expansion함 // 여기서는 SCA(채널쪼개서 서로 곱하는)부분에서 발생하는 non-linearity에서의 정보유실 방지\n",
    "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=dw_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(in_channels=dw_channel, out_channels=dw_channel, kernel_size=3, padding=1, stride=1, groups=dw_channel,\n",
    "                               bias=True)\n",
    "        self.conv3 = nn.Conv2d(in_channels=dw_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
    "        \n",
    "        # Simplified Channel Attention\n",
    "        # GAP진행해서 Channel별로 정보 뽑고, Point-wise Conv 진행후에, 원래 feature와 channel-wise multiplication 진행\n",
    "        self.sca = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels=dw_channel // 2, out_channels=dw_channel // 2, kernel_size=1, padding=0, stride=1,\n",
    "                      groups=1, bias=True),\n",
    "        )\n",
    "        \n",
    "        # 2@@@추가한부분, MLP인 1x1 point conv를 1d-conv로 교체했음. // 매우 빠르다는 장점도 있고, ECA에서 성능향상도 있었음. 기존의 SE Block을 대체하기 위해 나온 개념.\n",
    "        #---------------------------------------------------#\n",
    "        t = int(abs((m.log(dw_channel//2, 2) + 1) / 2))\n",
    "        self.k_size = t if t % 2 else t + 1 # k_size가 홀수여야 padding이 가능함.\n",
    "        self.sca1 = nn.AdaptiveAvgPool2d(1)\n",
    "        self.sca2 = nn.Conv1d(in_channels=dw_channel // 2, out_channels=dw_channel // 2, kernel_size = self.k_size, padding=(self.k_size-1)//2, bias=False)   #@@@ 추가한 부분\n",
    "        #---------------------------------------------------#\n",
    "\n",
    "        # SimpleGate\n",
    "        self.sg = SimpleGate()\n",
    "\n",
    "        ffn_channel = FFN_Expand * c\n",
    "        self.conv4 = nn.Conv2d(in_channels=c, out_channels=ffn_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
    "        self.conv5 = nn.Conv2d(in_channels=ffn_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
    "\n",
    "        self.norm1 = LayerNorm2d(c, eps=1e-6)\n",
    "        self.norm2 = LayerNorm2d(c, eps=1e-6)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
    "        self.dropout2 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
    "\n",
    "        # Normalization 파라미터, 단순히 정규화해서 ReLU를 통과시키면, 대부분의 Param이 소멸되므로, beta와 같은 bias 추가\n",
    "        self.beta = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
    "        self.gamma = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = inp\n",
    "\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x = self.conv1(x)   # Expansion (point-wise conv)\n",
    "        x = self.conv2(x)   # Depthwise Conv\n",
    "        x = self.sg(x)      # 채널 쪼개서 곱하기 for non-linearity / 이 과정에서 Channel수가 반으로 줄음\n",
    "        # x = x * self.sca(x) # Channel-wise Attension (element-wise mult)\n",
    "        # 2@@@추가한부분, MLP인 1x1 point conv를 1d-conv로 교체했음. // 매우 빠르다는 장점도 있고, ECA에서 성능향상도 있었음. 기존의 SE Block을 대체하기 위해 나온 개념.\n",
    "        #---------------------------------------------------#\n",
    "        x = self.sca1(x)\n",
    "        x = self.sca2(x.squeeze(-1)).unsqueeze(-1)  # conv1d는 [Batch_N, Channel, Length] 형식으로 받기때문에 기존의 [Batch_N, Channel, H, W]를 수정해줘야함\n",
    "        #---------------------------------------------------#\n",
    "        x = self.conv3(x)   # 원래 크기로 복구 (point-wise conv)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        y = inp + x * self.beta\n",
    "\n",
    "        x = self.conv4(self.norm2(y))   # 채널 수 ffn_channel으로 뻥튀기 (point-wise conv)\n",
    "        x = self.sg(x)      # 채널 수 반(ffn_channel // 2)으로 줄음\n",
    "        x = self.conv5(x)   # 원래 채널 수로 복구 (point-wise conv)\n",
    "\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        return y + x * self.gamma\n",
    "\n",
    "class NAFNet(nn.Module):\n",
    "\n",
    "    def __init__(self, img_channel=3, width=32, middle_blk_num=1, enc_blk_nums=[], dec_blk_nums=[]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.intro = nn.Conv2d(in_channels=img_channel, out_channels=width, kernel_size=3, padding=1, stride=1, groups=1,\n",
    "                              bias=True)\n",
    "        self.ending = nn.Conv2d(in_channels=width, out_channels=img_channel, kernel_size=3, padding=1, stride=1, groups=1,\n",
    "                              bias=True)    # 최종적으로는 채널 3으로 나가야하니깐.\n",
    "\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.decoders = nn.ModuleList()\n",
    "        self.middle_blks = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "\n",
    "        chan = width\n",
    "        for num in enc_blk_nums:    # [1, 1, 1, 28]\n",
    "            self.encoders.append(\n",
    "                nn.Sequential(\n",
    "                    *[NAFBlock(chan) for _ in range(num)]   # 변수 num(1~28)이 여기에만 붙어있어. 즉 얘는 28번 반복. 하지만 down은 len(list) == 4번만 진행\n",
    "                )\n",
    "            )\n",
    "            self.downs.append(      # len(enc_blk_nums) == 4, 즉 2**4 배 만큼 이미지크기 감소\n",
    "                nn.Conv2d(chan, 2*chan, kernel_size=2, stride=2)    # 이미지 크기는 절반 / 채널은 2 배\n",
    "            )\n",
    "            chan = chan * 2\n",
    "\n",
    "        self.middle_blks = \\\n",
    "            nn.Sequential(\n",
    "                *[NAFBlock(chan) for _ in range(middle_blk_num)]\n",
    "            )\n",
    "\n",
    "        for num in dec_blk_nums:\n",
    "            self.ups.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(chan, chan*2, kernel_size=1, bias=False),   # point-wise conv. 채널 수 2배 뻥튀기\n",
    "                    \n",
    "                    nn.PixelShuffle(upscale_factor=2)   # Feature map의 수많은 channel을 이용하여, pixel 위치에 맞는 각 Channel의 값을 떼어와서 feature map 확장  # 논문 참고 (https://mole-starseeker.tistory.com/m/84)\n",
    "                                                        # 이미지를 가로 세로 2배씩 확장한다면, 채널은 4개가 필요함 // 따라서 채널이 upscale_factor ** 2 만큼 감소\n",
    "                )\n",
    "            )\n",
    "            chan = chan // 2    # 채널 수 다시 2배 감소\n",
    "            self.decoders.append(\n",
    "                nn.Sequential(\n",
    "                    *[NAFBlock(chan) for _ in range(num)]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.padder_size = 2 ** len(self.encoders)  # == len(enc_blk_nums) == 4, down으로 인해 감소된 배수\n",
    "\n",
    "    def forward(self, inp):\n",
    "        B, C, H, W = inp.shape\n",
    "        inp = self.check_image_size(inp)    # padding 안함\n",
    "\n",
    "        x = self.intro(inp)\n",
    "\n",
    "        encs = []\n",
    "\n",
    "\n",
    "        # [1, 1, 1, 28] 이 었으니, 4번만큼 반복\n",
    "        for encoder, down in zip(self.encoders, self.downs):\n",
    "            x = encoder(x)\n",
    "            encs.append(x)\n",
    "            x = down(x)\n",
    "\n",
    "        x = self.middle_blks(x)\n",
    "\n",
    "        for decoder, up, enc_skip in zip(self.decoders, self.ups, encs[::-1]):\n",
    "            x = up(x)           # enc와 채널개수 및 이미지 크기를 맞춤\n",
    "            x = x + enc_skip    # skip-connection 이용\n",
    "            x = decoder(x)      # \n",
    "\n",
    "        x = self.ending(x)\n",
    "        x = x + inp\n",
    "\n",
    "        return x[:, :, :H, :W]\n",
    "\n",
    "    def check_image_size(self, x):\n",
    "        _, _, h, w = x.size()\n",
    "        mod_pad_h = (self.padder_size - h % self.padder_size) % self.padder_size\n",
    "        mod_pad_w = (self.padder_size - w % self.padder_size) % self.padder_size\n",
    "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRandomCrop(nn.Module):\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_params(img, output_size=(256, 256)):\n",
    "        w, h = TVF._get_image_size(img)\n",
    "        th, tw = output_size\n",
    "\n",
    "        if h + 1 < th or w + 1 < tw:\n",
    "            raise ValueError(\n",
    "                \"Required crop size {} is larger then input image size {}\".format((th, tw), (h, w))\n",
    "            )\n",
    "\n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "\n",
    "        i = torch.randint(0, h - th + 1, size=(1, )).item()\n",
    "        j = torch.randint(0, w - tw + 1, size=(1, )).item()\n",
    "        return i, j, th, tw\n",
    "\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, images):\n",
    "        lq, gt = images['lq'], images['gt']\n",
    "\n",
    "        width, height = TVF._get_image_size(gt)\n",
    "\n",
    "        i, j, h, w = self.get_params(gt, self.size)\n",
    "\n",
    "        return {'lq':TVF.crop(lq, i, j, h, w), 'gt':TVF.crop(gt, i, j, h, w)}\n",
    "\n",
    "class CustomRandomHorizontalFlip():\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, images):\n",
    "        lq, gt = images['lq'], images['gt']\n",
    "        if torch.rand(1) < self.p:\n",
    "            return {'lq': TVF.hflip(lq), 'gt': TVF.hflip(gt)}\n",
    "        return images\n",
    "\n",
    "class CustomRandomRotation():\n",
    "    def __init__(self, degrees=90, p=0.5):\n",
    "        self.degrees = degrees\n",
    "        self.p = p\n",
    "  \n",
    "    def __call__(self, images):\n",
    "        lq, gt = images['lq'], images['gt']\n",
    "        if torch.rand(1) < self.p:\n",
    "            return {'lq': TVF.rotate(lq, angle=self.degrees), 'gt':TVF.rotate(gt, angle=self.degrees)}\n",
    "        return images\n",
    "\n",
    "class CustomToTensor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, images):\n",
    "        lq, gt = images['lq'], images['gt']\n",
    "        return {'lq': TVF.to_tensor(lq), 'gt': TVF.to_tensor(gt)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoPro(Dataset):\n",
    "    def __init__(self, root='./dataset', mode='train', new_datasets=False):\n",
    "        super(GoPro, self).__init__()\n",
    "        # 1. 이미지들의 경로 저장\n",
    "        # 2. 이미지 전처리 옵션 설정\n",
    "        if new_datasets:\n",
    "            if mode == 'train':\n",
    "                self.lqs = sorted(glob(f'{root}/new_{mode}/blur_crops/*'))\n",
    "                self.gts = sorted(glob(f'{root}/new_{mode}/sharp_crops/*'))\n",
    "            elif mode == 'test':\n",
    "                self.lqs = sorted(glob(f'{root}/{mode}/*/blur/*'))\n",
    "                self.gts = sorted(glob(f'{root}/{mode}/*/sharp/*'))\n",
    "        else:\n",
    "            self.lqs = sorted(glob(f'{root}/{mode}/*/blur/*'))\n",
    "            self.gts = sorted(glob(f'{root}/{mode}/*/sharp/*'))\n",
    "            \n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "\n",
    "        self.transform_train = transforms.Compose([\n",
    "            # CustomToTensor(),\n",
    "            CustomRandomHorizontalFlip(p=0.5), # 다양한 이미지를 추출하기 위해 적용\n",
    "            CustomRandomRotation(degrees=90),    # 256 정방형으로 크기를 고정하고 회전시키는게 좋을거같다는 판단ㄴ\n",
    "            CustomRandomCrop((256, 256)), # @@@ Resize는 사진을 변형시켜서, 나중에 256x256으로 resize한 사진에만 훈련효과를 볼수있음\n",
    "        ])\n",
    "\n",
    "        self.transform_test = transforms.Compose([\n",
    "            # transforms.CenterCrop(224),\n",
    "            CustomRandomCrop((256, 256)), # @@@ Resize는 사진을 변형시켜서, 나중에 256x256으로 resize한 사진에만 훈련효과를 볼수있음\n",
    "        ])\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        lq = transforms.ToTensor()(Image.open(self.lqs[index]).convert('RGB'))\n",
    "        gt = transforms.ToTensor()(Image.open(self.gts[index]).convert('RGB'))\n",
    "        \n",
    "        if self.mode == 'train': images = self.transform_train({'lq':lq, 'gt':gt})\n",
    "        elif self.mode == 'test': images = self.transform_test({'lq':lq, 'gt':gt})\n",
    "            \n",
    "        return images['lq'], images['gt']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSNRLoss(nn.Module):\n",
    "    def __init__(self, loss_weight=1.0, reduction='mean'):\n",
    "        super(PSNRLoss, self).__init__()\n",
    "        assert reduction == 'mean'\n",
    "        self.loss_weight = loss_weight\n",
    "        self.scale = 10 / np.log(10)\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        return self.loss_weight * self.scale * torch.log(((pred - target) ** 2).mean(dim=(1, 2, 3)) + 1e-8).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "batch_size = 4\n",
    "shuffle = True\n",
    "\n",
    "# Network\n",
    "img_channel = 3\n",
    "width = 32\n",
    "enc_blks = [1, 1, 1, 28]\n",
    "middle_blk_num = 1\n",
    "dec_blks = [1, 1, 1, 1]\n",
    "\n",
    "# Loss\n",
    "losses = ['L1Loss', 'PSNRLoss']\n",
    "loss = 'L1Loss_1dconv'\n",
    "loss_folders = [f'd:/checkpoint/{loss}', f'd:/img/result/{loss}']\n",
    "for loss_folder in loss_folders:\n",
    "    if not os.path.exists(loss_folder): os.makedirs(loss_folder)\n",
    "\n",
    "# Train\n",
    "resume_train = True\n",
    "try: curr_epoch = list(map(lambda x : x[-8:-4], glob(f'd:/checkpoint/{loss}/*')))[-1]\n",
    "except: curr_epoch = '0000'; resume_train = False\n",
    "epochs = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloading\n",
    "train_ds = GoPro(root='./dataset', mode='train', new_datasets=True)\n",
    "test_ds = GoPro(root='./dataset', mode='test')\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True, drop_last=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "# Network\n",
    "net = NAFNet(img_channel=img_channel, width=width, middle_blk_num=middle_blk_num,\n",
    "                    enc_blk_nums=enc_blks, dec_blk_nums=dec_blks)\n",
    "net.to(device)\n",
    "\n",
    "# Loss\n",
    "if loss == 'L1Loss':\n",
    "    cri_pix = nn.L1Loss()\n",
    "elif loss == 'PSNRLoss':\n",
    "    cri_pix = PSNRLoss()\n",
    "else:\n",
    "    # loss = 'L1Loss'\n",
    "    cri_pix = nn.L1Loss()\n",
    "cri_pix.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer_g = torch.optim.AdamW(net.parameters(), lr=1e-3, weight_decay=1e-3, betas=(0.9, 0.9))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_g, T_max=int(len(train_ds)/batch_size), eta_min=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset check\n",
    "lq, gt = next(iter(train_dl))\n",
    "img_sample = torch.cat((lq, gt), -2) # 높이(height)를 기준으로 이미지를 연결하기\n",
    "save_image(img_sample, \"train_dl_img.png\", nrow=batch_size, normalize=False)\n",
    "\n",
    "# Network check\n",
    "from torchsummary import summary\n",
    "summary(net,(3,256,256),batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: curr_epoch = list(map(lambda x : x[-8:-4], glob(f'd:/checkpoint/{loss}/*')))[-1]\n",
    "except: curr_epoch = '0000'; resume_train = False\n",
    "\n",
    "if resume_train:\n",
    "    state_dict = torch.load(f'd:/checkpoint/{loss}/checkpoint.{curr_epoch}.pth')\n",
    "\n",
    "    resume_epoch = state_dict['epoch'] + 1  # +1 부터 시작할 수 있게\n",
    "    optimizer_state_dict = state_dict['optimizer_state_dict']\n",
    "    model_state_dict = state_dict['model_state_dict']\n",
    "    scheduler_state_dict = state_dict['scheduler_state_dict']\n",
    "\n",
    "    print(f'resume_epoch: {resume_epoch}')\n",
    "\n",
    "    optimizer_g.load_state_dict(optimizer_state_dict)\n",
    "    net.load_state_dict(model_state_dict, strict=True)\n",
    "    scheduler.load_state_dict(scheduler_state_dict)\n",
    "else:\n",
    "    resume_epoch = 0\n",
    "\n",
    "for epoch in range(resume_epoch, epochs):\n",
    "    pbar = tqdm(train_dl)\n",
    "    for i, batch in enumerate(pbar):\n",
    "        lq, gt = batch\n",
    "        lq = lq.to(device)\n",
    "        gt = gt.to(device)\n",
    "        \n",
    "        optimizer_g.zero_grad()\n",
    "\n",
    "        preds_t = net(lq)\n",
    "        if not isinstance(preds_t, list): # preds가 tensor라서 이걸 list로 바꿔주는거\n",
    "            preds = [preds_t]\n",
    "\n",
    "        output = preds[-1]\n",
    "\n",
    "        l_total = 0\n",
    "        # pixel loss\n",
    "        if cri_pix:\n",
    "            l_pix = 0.\n",
    "            for pred in preds:\n",
    "                l_pix += cri_pix(pred, gt)   # 누적연산은 좋지않댔는데, float를 붙이면 해결가능함 (https://pytorch.org/docs/stable/notes/faq.html#my-model-reports-cuda-runtime-error-2-out-of-memory)\n",
    "            l_total += l_pix\n",
    "        \n",
    "        l_total = l_total + 0. * sum(p.sum() for p in net.parameters())\n",
    "        pbar.set_postfix_str(f\"epoch: {epoch}, l_pix: {l_pix}, lr: {optimizer_g.param_groups[0]['lr']}\")\n",
    "\n",
    "        l_total.backward()\n",
    "        use_grad_clip = True\n",
    "        if use_grad_clip:\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 0.01)\n",
    "        optimizer_g.step()\n",
    "        \n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    img_sample = torch.cat((lq.data, preds_t, gt.data), -2) # 높이(height)를 기준으로 이미지를 연결하기\n",
    "    save_image(img_sample, f\"d:/img/result/{loss}/result{epoch:04d}.png\", nrow=batch_size, normalize=False)\n",
    "    print(f\"[Sample img 저장 완료] d:/img/result/{loss}/result{epoch:04d}.png\")\n",
    "    \n",
    "    if epoch % 3 == 0:\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'optimizer_state_dict': optimizer_g.state_dict(),\n",
    "                'scheduler_state_dict':scheduler.state_dict(),\n",
    "                'model_state_dict': net.state_dict(),\n",
    "            }, f'd:/checkpoint/{loss}/checkpoint.{epoch:04d}.pth')\n",
    "        print(f'[Check point 저장 완료] d:/checkpoint/{loss}/checkpoint.{epoch:04d}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint0078를 이용하여 Inference 진행\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    isinstance(net, NAFNet)\n",
    "except:\n",
    "    net = NAFNet(img_channel=img_channel, width=width, middle_blk_num=middle_blk_num,\n",
    "                    enc_blk_nums=enc_blks, dec_blk_nums=dec_blks)\n",
    "    net.to(device)\n",
    "\n",
    "try: curr_epoch = list(map(lambda x : x[-8:-4], glob(f'd:/checkpoint/{loss}/*')))[-1]; print(f'checkpoint{curr_epoch}를 이용하여 Inference 진행')\n",
    "except: raise Exception(f'd:/checkpoint/{loss}/ 에 체크포인트가 없습니다.')\n",
    "\n",
    "state_dict = torch.load(f'd:/checkpoint/{loss}/checkpoint.{curr_epoch}.pth')\n",
    "model_state_dict = state_dict['model_state_dict']\n",
    "net.load_state_dict(model_state_dict, strict=True)  # 왜 넣어야하는거지?\n",
    "net.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 이미지 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[저장 완료] img/result/L1Loss/result0099_test.png\n"
     ]
    }
   ],
   "source": [
    "lq, gt = next(iter(test_dl))\n",
    "lq = lq.to(device)\n",
    "gt = gt.to(device)\n",
    "img_res = net(lq)\n",
    "\n",
    "img_sample = torch.cat((lq.data, img_res.data, gt.data), -2) # 높이(height)를 기준으로 이미지를 연결하기\n",
    "save_image(img_sample, f\"img/result/{loss}/result{curr_epoch}_test.png\", nrow=batch_size, normalize=False)\n",
    "print(f\"[저장 완료] img/result/{loss}/result{curr_epoch}_test.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSNR 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 171/277 [01:09<00:42,  2.47it/s, current psnr_avg: 24.94337478803366] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m sum_psnr \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      6\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(test_dl)\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(pbar):\n\u001b[0;32m      8\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     pbar\u001b[39m.\u001b[39mset_postfix_str(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcurrent psnr_avg: \u001b[39m\u001b[39m{\u001b[39;00mpsnr_avg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\KUEEE04\\anaconda3\\envs\\yolo\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KUEEE04\\anaconda3\\envs\\yolo\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 521\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    524\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\KUEEE04\\anaconda3\\envs\\yolo\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    560\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    562\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    563\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Users\\KUEEE04\\anaconda3\\envs\\yolo\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:44\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 44\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     45\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\KUEEE04\\anaconda3\\envs\\yolo\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 44\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     45\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn [5], line 31\u001b[0m, in \u001b[0;36mGoPro.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m     30\u001b[0m     lq \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mToTensor()(Image\u001b[39m.\u001b[39mopen(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlqs[index])\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m---> 31\u001b[0m     gt \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mToTensor()(Image\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgts[index])\u001b[39m.\u001b[39;49mconvert(\u001b[39m'\u001b[39;49m\u001b[39mRGB\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     33\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m: images \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_train({\u001b[39m'\u001b[39m\u001b[39mlq\u001b[39m\u001b[39m'\u001b[39m:lq, \u001b[39m'\u001b[39m\u001b[39mgt\u001b[39m\u001b[39m'\u001b[39m:gt})\n\u001b[0;32m     34\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m: images \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_test({\u001b[39m'\u001b[39m\u001b[39mlq\u001b[39m\u001b[39m'\u001b[39m:lq, \u001b[39m'\u001b[39m\u001b[39mgt\u001b[39m\u001b[39m'\u001b[39m:gt})\n",
      "File \u001b[1;32mc:\\Users\\KUEEE04\\anaconda3\\envs\\yolo\\lib\\site-packages\\PIL\\Image.py:889\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, matrix\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dither\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, palette\u001b[39m=\u001b[39mWEB, colors\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m):\n\u001b[0;32m    848\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[39m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    850\u001b[0m \u001b[39m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[39m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    891\u001b[0m     has_transparency \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtransparency\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    892\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mode \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    893\u001b[0m         \u001b[39m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\KUEEE04\\anaconda3\\envs\\yolo\\lib\\site-packages\\PIL\\ImageFile.py:253\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    248\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mimage file is truncated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(b)\u001b[39m}\u001b[39;00m\u001b[39m bytes not processed)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n\u001b[0;32m    252\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[1;32m--> 253\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[0;32m    254\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    255\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils.psnr_measure import psnr\n",
    "\n",
    "psnr_avg = 0\n",
    "sum_psnr = 0\n",
    "\n",
    "pbar = tqdm(test_dl)\n",
    "for i, batch in enumerate(pbar):\n",
    "    if i == 0: continue\n",
    "    pbar.set_postfix_str(f\"current psnr_avg: {psnr_avg}\")\n",
    "    \n",
    "    lq, gt = batch\n",
    "    lq = lq.to(device)\n",
    "    gt = gt.to(device)\n",
    "    results = net(lq)\n",
    "    for result in results:\n",
    "        sum_psnr += psnr(result.cpu().detach().numpy(), gt.cpu().detach().numpy())\n",
    "    psnr_avg = sum_psnr / (i * batch_size)\n",
    "\n",
    "print(psnr_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference With CPU\n",
    "demo_image = transforms.ToTensor()(Image.open('./blurry.png').convert(\"RGB\")).unsqueeze(dim=0)\n",
    "net.to('cpu')\n",
    "img_res = net(demo_image)\n",
    "save_image(img_res, f\"blurry_result2.png\", nrow=batch_size, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('yolo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff0323b92811708b19364feae987dc3a6297456b8196963308283135522dbebe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
