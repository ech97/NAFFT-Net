{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 관리\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# 이미지 처리\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# For Custom Dataset\n",
    "import torchvision.transforms.functional as TVF\n",
    "\n",
    "# Network\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch._jit_internal import Optional\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "# For Custom Network\n",
    "import math as m\n",
    "# Utils\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO-Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DOConv2d(nn.Module):\n",
    "    # jit의 최적화를 위한 변수 선언\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
    "                     'padding_mode', 'output_padding', 'in_channels',\n",
    "                     'out_channels', 'kernel_size', 'D_mul']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, D_mul=None, stride=1,\n",
    "                 padding=1, dilation=1, groups=1, bias=False, padding_mode='zeros', simam=False):\n",
    "        super(DOConv2d, self).__init__()\n",
    "\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "        stride = (stride, stride)\n",
    "        padding = (padding, padding)\n",
    "        dilation = (dilation, dilation)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        self._padding_repeated_twice = tuple(x for x in self.padding for _ in range(2))\n",
    "        self.simam = simam\n",
    "        M = self.kernel_size[0]\n",
    "        N = self.kernel_size[1]\n",
    "        self.D_mul = M * N if D_mul is None or M * N <= 1 else D_mul\n",
    "        self.W = Parameter(torch.Tensor(out_channels, in_channels // groups, self.D_mul))\n",
    "        init.kaiming_uniform_(self.W, a=m.sqrt(5))\n",
    "\n",
    "        if M * N > 1:\n",
    "            self.D = Parameter(torch.Tensor(in_channels, M * N, self.D_mul))\n",
    "            init_zero = np.zeros([in_channels, M * N, self.D_mul], dtype=np.float32)\n",
    "            self.D.data = torch.from_numpy(init_zero)\n",
    "\n",
    "            eye = torch.reshape(torch.eye(M * N, dtype=torch.float32), (1, M * N, M * N))\n",
    "            D_diag = eye.repeat((in_channels, 1, self.D_mul // (M * N)))\n",
    "            if self.D_mul % (M * N) != 0:  # the cases when D_mul > M * N\n",
    "                zeros = torch.zeros([in_channels, M * N, self.D_mul % (M * N)])\n",
    "                self.D_diag = Parameter(torch.cat([D_diag, zeros], dim=2), requires_grad=False)\n",
    "            else:  # the case when D_mul = M * N\n",
    "                self.D_diag = Parameter(D_diag, requires_grad=False)\n",
    "        if simam:\n",
    "            self.simam_block = simam_module()\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.W)\n",
    "            bound = 1 / m.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(DOConv2d, self).__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "\n",
    "    def _conv_forward(self, input, weight):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, self.bias, self.stride,\n",
    "                            (0, 0), self.dilation, self.groups)\n",
    "        return F.conv2d(input, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input):\n",
    "        M = self.kernel_size[0]\n",
    "        N = self.kernel_size[1]\n",
    "        DoW_shape = (self.out_channels, self.in_channels // self.groups, M, N)\n",
    "        if M * N > 1:\n",
    "            # D W 계산\n",
    "            # (input_channels, D_mul, M * N)\n",
    "            D = self.D + self.D_diag\n",
    "            W = torch.reshape(self.W, (self.out_channels // self.groups, self.in_channels, self.D_mul))\n",
    "\n",
    "            # einsum outputs (out_channels // groups, in_channels, M * N)는 다음과같이 Reshape됨 (out_channels, in_channels // groups, M, N)\n",
    "            DoW = torch.reshape(torch.einsum('ims,ois->oim', D, W), DoW_shape)\n",
    "        else:\n",
    "            DoW = torch.reshape(self.W, DoW_shape)\n",
    "        if self.simam:\n",
    "            DoW_h1, DoW_h2 = torch.chunk(DoW, 2, dim=2)\n",
    "            DoW = torch.cat([self.simam_block(DoW_h1), DoW_h2], dim=2)\n",
    "\n",
    "        return self._conv_forward(input, DoW)\n",
    "\n",
    "class simam_module(torch.nn.Module):\n",
    "    def __init__(self, e_lambda=1e-4):\n",
    "        super(simam_module, self).__init__()\n",
    "        self.activaton = nn.Sigmoid()\n",
    "        self.e_lambda = e_lambda\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        n = w * h - 1\n",
    "        x_minus_mu_square = (x - x.mean(dim=[2, 3], keepdim=True)).pow(2)\n",
    "        y = x_minus_mu_square / (4 * (x_minus_mu_square.sum(dim=[2, 3], keepdim=True) / n + self.e_lambda)) + 0.5\n",
    "        return x * self.activaton(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LayerNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight, bias, eps):\n",
    "        ctx.eps = eps\n",
    "        N, C, H, W = x.size()\n",
    "        mu = x.mean(1, keepdim=True)\n",
    "        var = (x - mu).pow(2).mean(1, keepdim=True)\n",
    "        y = (x - mu) / (var + eps).sqrt()\n",
    "        ctx.save_for_backward(y, var, weight)\n",
    "        y = weight.view(1, C, 1, 1) * y + bias.view(1, C, 1, 1)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        eps = ctx.eps\n",
    "\n",
    "        N, C, H, W = grad_output.size()\n",
    "        y, var, weight = ctx.saved_tensors\n",
    "        g = grad_output * weight.view(1, C, 1, 1)\n",
    "        mean_g = g.mean(dim=1, keepdim=True)\n",
    "\n",
    "        mean_gy = (g * y).mean(dim=1, keepdim=True)\n",
    "        gx = 1. / torch.sqrt(var + eps) * (g - y * mean_gy - mean_g)\n",
    "        return gx, (grad_output * y).sum(dim=3).sum(dim=2).sum(dim=0), grad_output.sum(dim=3).sum(dim=2).sum(\n",
    "            dim=0), None\n",
    "\n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, channels, eps=1e-6):\n",
    "        super(LayerNorm2d, self).__init__()\n",
    "        self.register_parameter('weight', nn.Parameter(torch.ones(channels)))\n",
    "        self.register_parameter('bias', nn.Parameter(torch.zeros(channels)))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return LayerNormFunction.apply(x, self.weight, self.bias, self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAFFTNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTBlock(nn.Module):\n",
    "    def __init__(self, out_channel, norm='backward'):\n",
    "        super(FFTBlock, self).__init__()\n",
    "        self.main_fft = nn.Sequential(\n",
    "            DOConv2d(out_channel*2, out_channel*2, kernel_size=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            DOConv2d(out_channel*2, out_channel*2, kernel_size=1, stride=1)\n",
    "        )\n",
    "        self.norm = norm\n",
    "    def forward(self, x):\n",
    "        _, C, H, W = x.shape\n",
    "        dim = 1\n",
    "        y = torch.fft.rfft2(x, norm=self.norm)\n",
    "        y_imag = y.imag\n",
    "        y_real = y.real\n",
    "        y_f = torch.cat([y_real, y_imag], dim=dim)\n",
    "        \n",
    "        y = self.main_fft(y_f)\n",
    "        \n",
    "        y_real, y_imag = torch.chunk(y, 2, dim=dim)\n",
    "        y = torch.complex(y_real, y_imag)\n",
    "        y = torch.fft.irfft2(y, s=(H, W), norm=self.norm)\n",
    "        return x + y    # 입력으로 들어온 값과, FFT층 통과한 이미지 더해서 return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel 방향으로 쪼개서 Element-wise mul 진행\n",
    "class SimpleGate(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x1, x2 = x.chunk(2, dim=1)  # (batch, channel, row, col)\n",
    "        return x1 * x2\n",
    "\n",
    "class NAFBlock(nn.Module):\n",
    "    def __init__(self, c, DW_Expand=2, FFN_Expand=2, drop_out_rate=0.):\n",
    "        super(NAFBlock, self).__init__()\n",
    "        dw_channel = c * DW_Expand  # Embedding 과정과, Non-linear 과정에서 정보유실을 방지하기위해 Expansion함 // 여기서는 SCA(채널쪼개서 서로 곱하는)부분에서 발생하는 non-linearity에서의 정보유실 방지\n",
    "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=dw_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
    "        self.conv2 = DOConv2d(in_channels=dw_channel, out_channels=dw_channel, kernel_size=3, padding=1, stride=1, groups=dw_channel,\n",
    "                               bias=True)\n",
    "        self.conv3 = nn.Conv2d(in_channels=dw_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
    "        \n",
    "        # Simplified Channel Attention\n",
    "        # GAP진행해서 Channel별로 정보 뽑고, Point-wise Conv 진행후에, 원래 feature와 channel-wise multiplication 진행\n",
    "        self.sca = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels=dw_channel // 2, out_channels=dw_channel // 2, kernel_size=1, padding=0, stride=1,\n",
    "                      groups=1, bias=True),\n",
    "        )\n",
    "        \n",
    "        # 2@@@추가한부분, MLP인 1x1 point conv를 1d-conv로 교체했음. // 매우 빠르다는 장점도 있고, ECA에서 성능향상도 있었음. 기존의 SE Block을 대체하기 위해 나온 개념.\n",
    "        #---------------------------------------------------#\n",
    "        t = int(abs((m.log(dw_channel//2, 2) + 1) / 2))\n",
    "        self.k_size = t if t % 2 else t + 1 # k_size가 홀수여야 padding이 가능함.\n",
    "        self.sca1 = nn.AdaptiveAvgPool2d(1)\n",
    "        self.sca2 = nn.Conv1d(in_channels=dw_channel // 2, out_channels=dw_channel // 2, kernel_size = self.k_size, padding=(self.k_size-1)//2, bias=False)   #@@@ 추가한 부분\n",
    "        #---------------------------------------------------#\n",
    "\n",
    "        # SimpleGate\n",
    "        self.sg = SimpleGate()\n",
    "\n",
    "        ffn_channel = FFN_Expand * c\n",
    "        self.conv4 = nn.Conv2d(in_channels=c, out_channels=ffn_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
    "        self.conv5 = nn.Conv2d(in_channels=ffn_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
    "\n",
    "        self.norm1 = LayerNorm2d(c, eps=1e-6)\n",
    "        self.norm2 = LayerNorm2d(c, eps=1e-6)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
    "        self.dropout2 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
    "\n",
    "        # Normalization 파라미터, 단순히 정규화해서 ReLU를 통과시키면, 대부분의 Param이 소멸되므로, beta와 같은 bias 추가\n",
    "        self.beta = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
    "        self.gamma = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = inp\n",
    "\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x = self.conv1(x)   # Expansion (point-wise conv)\n",
    "        x = self.conv2(x)   # Depthwise Conv\n",
    "        x = self.sg(x)      # 채널 쪼개서 곱하기 for non-linearity / 이 과정에서 Channel수가 반으로 줄음\n",
    "        # x = x * self.sca(x) # Channel-wise Attension (element-wise mult)\n",
    "        # 2@@@추가한부분, MLP인 1x1 point conv를 1d-conv로 교체했음. // 매우 빠르다는 장점도 있고, ECA에서 성능향상도 있었음. 기존의 SE Block을 대체하기 위해 나온 개념.\n",
    "        #---------------------------------------------------#\n",
    "        x = self.sca1(x)\n",
    "        x = self.sca2(x.squeeze(-1)).unsqueeze(-1)  # conv1d는 [Batch_N, Channel, Length] 형식으로 받기때문에 기존의 [Batch_N, Channel, H, W]를 수정해줘야함\n",
    "        #---------------------------------------------------#\n",
    "        x = self.conv3(x)   # 원래 크기로 복구 (point-wise conv)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        y = inp + x * self.beta\n",
    "\n",
    "        x = self.conv4(self.norm2(y))   # 채널 수 ffn_channel으로 뻥튀기 (point-wise conv)\n",
    "        x = self.sg(x)      # 채널 수 반(ffn_channel // 2)으로 줄음\n",
    "        x = self.conv5(x)   # 원래 채널 수로 복구 (point-wise conv)\n",
    "\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        return y + x * self.gamma\n",
    "\n",
    "class NAFFTNet(nn.Module):\n",
    "    def __init__(self, img_channel=3, width=32, middle_blk_num=1, enc_blk_nums=[], dec_blk_nums=[]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.intro = nn.Conv2d(in_channels=img_channel, out_channels=width, kernel_size=3, padding=1, stride=1, groups=1,\n",
    "                              bias=True)\n",
    "        self.ending = nn.Conv2d(in_channels=width, out_channels=img_channel, kernel_size=3, padding=1, stride=1, groups=1,\n",
    "                              bias=True)    # 최종적으로는 채널 3으로 나가야하니깐.\n",
    "\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.decoders = nn.ModuleList()\n",
    "        self.middle_blks = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        #------------------------------------------------------------------#\n",
    "        self.fftBlock = nn.ModuleList() \n",
    "        #------------------------------------------------------------------#\n",
    "\n",
    "        chan = width\n",
    "        for num in enc_blk_nums:    # [1, 1, 1, 28]\n",
    "            self.encoders.append(\n",
    "                nn.Sequential(\n",
    "                    *[NAFBlock(chan) for _ in range(num)]   # 변수 num(1~28)이 여기에만 붙어있어. 즉 얘는 28번 반복. 하지만 down은 len(list) == 4번만 진행\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            self.fftBlock.append(\n",
    "                nn.Sequential(\n",
    "                    FFTBlock(chan)\n",
    "                )\n",
    "            )\n",
    "            self.downs.append(      # len(enc_blk_nums) == 4, 즉 2**4 배 만큼 이미지크기 감소\n",
    "                nn.Conv2d(chan, 2*chan, kernel_size=2, stride=2)    # 이미지 크기는 절반 / 채널은 2 배\n",
    "            )\n",
    "            chan = chan * 2\n",
    "\n",
    "        self.middle_blks = \\\n",
    "            nn.Sequential(\n",
    "                *[NAFBlock(chan) for _ in range(middle_blk_num)]\n",
    "            )\n",
    "\n",
    "        for num in dec_blk_nums:\n",
    "            self.ups.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(chan, chan*2, kernel_size=1, bias=False),   # point-wise conv. 채널 수 2배 뻥튀기\n",
    "                    \n",
    "                    nn.PixelShuffle(upscale_factor=2)   # Feature map의 수많은 channel을 이용하여, pixel 위치에 맞는 각 Channel의 값을 떼어와서 feature map 확장  # 논문 참고 (https://mole-starseeker.tistory.com/m/84)\n",
    "                                                        # 이미지를 가로 세로 2배씩 확장한다면, 채널은 4개가 필요함 // 따라서 채널이 upscale_factor ** 2 만큼 감소\n",
    "                )\n",
    "            )\n",
    "            chan = chan // 2    # 채널 수 다시 2배 감소\n",
    "            self.decoders.append(\n",
    "                nn.Sequential(\n",
    "                    *[NAFBlock(chan) for _ in range(num)]\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "        self.padder_size = 2 ** len(self.encoders)  # == len(enc_blk_nums) == 4, down으로 인해 감소된 배수\n",
    "\n",
    "    def forward(self, inp):\n",
    "        B, C, H, W = inp.shape\n",
    "        inp = self.check_image_size(inp)    # padding 안함\n",
    "\n",
    "        x = self.intro(inp)\n",
    "\n",
    "        encs = []\n",
    "\n",
    "\n",
    "        # [1, 1, 1, 28] 이 었으니, 4번만큼 반복\n",
    "        for encoder, fftBlock, down in zip(self.encoders, self.fftBlock, self.downs):\n",
    "            x = encoder(x)\n",
    "            encs.append(fftBlock(x))\n",
    "            x = down(x)\n",
    "\n",
    "        x = self.middle_blks(x)\n",
    "\n",
    "        for decoder, up, enc_skip in zip(self.decoders, self.ups, encs[::-1]):\n",
    "            x = up(x)           # enc와 채널개수 및 이미지 크기를 맞춤\n",
    "            x = x + enc_skip    # skip-connection 이용\n",
    "            x = decoder(x)      # \n",
    "\n",
    "        x = self.ending(x)\n",
    "        x = x + inp\n",
    "\n",
    "        return x[:, :, :H, :W]\n",
    "\n",
    "    def check_image_size(self, x):\n",
    "        _, _, h, w = x.size()\n",
    "        mod_pad_h = (self.padder_size - h % self.padder_size) % self.padder_size\n",
    "        mod_pad_w = (self.padder_size - w % self.padder_size) % self.padder_size\n",
    "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRandomCrop(nn.Module):\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_params(img, output_size=(256, 256)):\n",
    "        w, h = TVF._get_image_size(img)\n",
    "        th, tw = output_size\n",
    "\n",
    "        if h + 1 < th or w + 1 < tw:\n",
    "            raise ValueError(\n",
    "                \"Required crop size {} is larger then input image size {}\".format((th, tw), (h, w))\n",
    "            )\n",
    "\n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "\n",
    "        i = torch.randint(0, h - th + 1, size=(1, )).item()\n",
    "        j = torch.randint(0, w - tw + 1, size=(1, )).item()\n",
    "        return i, j, th, tw\n",
    "\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, images):\n",
    "        lq, gt = images['lq'], images['gt']\n",
    "\n",
    "        width, height = TVF._get_image_size(gt)\n",
    "\n",
    "        i, j, h, w = self.get_params(gt, self.size)\n",
    "\n",
    "        return {'lq':TVF.crop(lq, i, j, h, w), 'gt':TVF.crop(gt, i, j, h, w)}\n",
    "\n",
    "class CustomRandomHorizontalFlip():\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, images):\n",
    "        lq, gt = images['lq'], images['gt']\n",
    "        if torch.rand(1) < self.p:\n",
    "            return {'lq': TVF.hflip(lq), 'gt': TVF.hflip(gt)}\n",
    "        return images\n",
    "\n",
    "class CustomRandomRotation():\n",
    "    def __init__(self, degrees=90, p=0.5):\n",
    "        self.degrees = degrees\n",
    "        self.p = p\n",
    "  \n",
    "    def __call__(self, images):\n",
    "        lq, gt = images['lq'], images['gt']\n",
    "        if torch.rand(1) < self.p:\n",
    "            return {'lq': TVF.rotate(lq, angle=self.degrees), 'gt':TVF.rotate(gt, angle=self.degrees)}\n",
    "        return images\n",
    "\n",
    "class CustomToTensor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, images):\n",
    "        lq, gt = images['lq'], images['gt']\n",
    "        return {'lq': TVF.to_tensor(lq), 'gt': TVF.to_tensor(gt)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoPro(Dataset):\n",
    "    def __init__(self, root='./dataset', mode='train', new_datasets=False):\n",
    "        super(GoPro, self).__init__()\n",
    "        # 1. 이미지들의 경로 저장\n",
    "        # 2. 이미지 전처리 옵션 설정\n",
    "        if new_datasets:\n",
    "            if mode == 'train':\n",
    "                self.lqs = sorted(glob(f'{root}/new_{mode}/blur_crops/*'))\n",
    "                self.gts = sorted(glob(f'{root}/new_{mode}/sharp_crops/*'))\n",
    "            elif mode == 'test':\n",
    "                self.lqs = sorted(glob(f'./testset/*'))\n",
    "                self.gts = sorted(glob(f'./testset/*'))\n",
    "        else:\n",
    "            self.lqs = sorted(glob(f'{root}/{mode}/*/blur/*'))\n",
    "            self.gts = sorted(glob(f'{root}/{mode}/*/sharp/*'))\n",
    "            \n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "\n",
    "        self.transform_train = transforms.Compose([\n",
    "            # CustomToTensor(),\n",
    "            CustomRandomHorizontalFlip(p=0.5), # 다양한 이미지를 추출하기 위해 적용\n",
    "            CustomRandomRotation(degrees=90),    # 256 정방형으로 크기를 고정하고 회전시키는게 좋을거같다는 판단ㄴ\n",
    "            CustomRandomCrop((256, 256)), # @@@ Resize는 사진을 변형시켜서, 나중에 256x256으로 resize한 사진에만 훈련효과를 볼수있음\n",
    "        ])\n",
    "\n",
    "        self.transform_test = transforms.Compose([\n",
    "            # transforms.CenterCrop(224),\n",
    "            # CustomRandomCrop((256, 256)), # @@@ Resize는 사진을 변형시켜서, 나중에 256x256으로 resize한 사진에만 훈련효과를 볼수있음\n",
    "        ])\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        lq = transforms.ToTensor()(Image.open(self.lqs[index]).convert('RGB'))\n",
    "        gt = transforms.ToTensor()(Image.open(self.gts[index]).convert('RGB'))\n",
    "        \n",
    "        if self.mode == 'train': images = self.transform_train({'lq':lq, 'gt':gt})\n",
    "        elif self.mode == 'test': images = self.transform_test({'lq':lq, 'gt':gt})\n",
    "            \n",
    "        return images['lq'], images['gt']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSNRLoss(nn.Module):\n",
    "    def __init__(self, loss_weight=1.0, reduction='mean'):\n",
    "        super(PSNRLoss, self).__init__()\n",
    "        assert reduction == 'mean'\n",
    "        self.loss_weight = loss_weight\n",
    "        self.scale = 10 / np.log(10)\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        return self.loss_weight * self.scale * torch.log(((pred - target) ** 2).mean(dim=(1, 2, 3)) + 1e-8).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFTLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        diff = torch.fft.fft2(input) - torch.fft.fft2(target)\n",
    "        loss = torch.mean(abs(diff))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "batch_size = 4\n",
    "test_batch_size = 1\n",
    "shuffle = True\n",
    "\n",
    "# Network\n",
    "img_channel = 3\n",
    "width = 32\n",
    "enc_blks = [1, 1, 1, 28]\n",
    "middle_blk_num = 1\n",
    "dec_blks = [1, 1, 1, 1]\n",
    "\n",
    "# Loss\n",
    "losses = ['L1Loss', 'PSNRLoss', 'FFTLoss']\n",
    "loss = 'L1Loss'\n",
    "loss_folders = [f'd:/checkpoint/{loss}', f'd:/img/result/{loss}']\n",
    "for loss_folder in loss_folders:\n",
    "    if not os.path.exists(loss_folder): os.makedirs(loss_folder)\n",
    "\n",
    "# Train\n",
    "resume_train = True\n",
    "try: curr_epoch = list(map(lambda x : x[-8:-4], glob(f'd:/checkpoint/{loss}/*')))[-1]\n",
    "except: curr_epoch = '0000'; resume_train = False\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloading\n",
    "train_ds = GoPro(root='./dataset', mode='train', new_datasets=True)\n",
    "test_ds = GoPro(root='./dataset', mode='test', new_datasets=True)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle, num_workers=0, pin_memory=True, drop_last=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=test_batch_size, shuffle=False, num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "# Network\n",
    "net = NAFFTNet(img_channel=img_channel, width=width, middle_blk_num=middle_blk_num,\n",
    "                    enc_blk_nums=enc_blks, dec_blk_nums=dec_blks)\n",
    "net.to(device)\n",
    "\n",
    "# Loss\n",
    "if loss == 'L1Loss':\n",
    "    cri_pix = nn.L1Loss()\n",
    "elif loss == 'PSNRLoss':\n",
    "    cri_pix = PSNRLoss()\n",
    "elif loss == 'FFTLoss':\n",
    "    # cri_pix = PSNRLoss()\n",
    "    cri_pix = nn.L1Loss()\n",
    "    cri_pix_fft = FFTLoss()\n",
    "    cri_pix_fft.to(device)\n",
    "else:\n",
    "    cri_pix = nn.L1Loss()\n",
    "\n",
    "cri_pix.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer_g = torch.optim.AdamW(net.parameters(), lr=1e-3, weight_decay=1e-3, betas=(0.9, 0.9))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_g, T_max=int(len(train_ds)/batch_size), eta_min=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [4, 32, 256, 256]             896\n",
      "       LayerNorm2d-2          [4, 32, 256, 256]              64\n",
      "            Conv2d-3          [4, 64, 256, 256]           2,112\n",
      "          DOConv2d-4          [4, 64, 256, 256]               0\n",
      "        SimpleGate-5          [4, 32, 256, 256]               0\n",
      " AdaptiveAvgPool2d-6              [4, 32, 1, 1]               0\n",
      "            Conv1d-7                 [4, 32, 1]           3,072\n",
      "            Conv2d-8              [4, 32, 1, 1]           1,056\n",
      "          Identity-9              [4, 32, 1, 1]               0\n",
      "      LayerNorm2d-10          [4, 32, 256, 256]              64\n",
      "           Conv2d-11          [4, 64, 256, 256]           2,112\n",
      "       SimpleGate-12          [4, 32, 256, 256]               0\n",
      "           Conv2d-13          [4, 32, 256, 256]           1,056\n",
      "         Identity-14          [4, 32, 256, 256]               0\n",
      "         NAFBlock-15          [4, 32, 256, 256]               0\n",
      "         DOConv2d-16          [4, 64, 258, 131]               0\n",
      "             ReLU-17          [4, 64, 258, 131]               0\n",
      "         DOConv2d-18          [4, 64, 260, 133]               0\n",
      "         FFTBlock-19          [4, 32, 256, 256]               0\n",
      "           Conv2d-20          [4, 64, 128, 128]           8,256\n",
      "      LayerNorm2d-21          [4, 64, 128, 128]             128\n",
      "           Conv2d-22         [4, 128, 128, 128]           8,320\n",
      "         DOConv2d-23         [4, 128, 128, 128]               0\n",
      "       SimpleGate-24          [4, 64, 128, 128]               0\n",
      "AdaptiveAvgPool2d-25              [4, 64, 1, 1]               0\n",
      "           Conv1d-26                 [4, 64, 1]          12,288\n",
      "           Conv2d-27              [4, 64, 1, 1]           4,160\n",
      "         Identity-28              [4, 64, 1, 1]               0\n",
      "      LayerNorm2d-29          [4, 64, 128, 128]             128\n",
      "           Conv2d-30         [4, 128, 128, 128]           8,320\n",
      "       SimpleGate-31          [4, 64, 128, 128]               0\n",
      "           Conv2d-32          [4, 64, 128, 128]           4,160\n",
      "         Identity-33          [4, 64, 128, 128]               0\n",
      "         NAFBlock-34          [4, 64, 128, 128]               0\n",
      "         DOConv2d-35          [4, 128, 130, 67]               0\n",
      "             ReLU-36          [4, 128, 130, 67]               0\n",
      "         DOConv2d-37          [4, 128, 132, 69]               0\n",
      "         FFTBlock-38          [4, 64, 128, 128]               0\n",
      "           Conv2d-39           [4, 128, 64, 64]          32,896\n",
      "      LayerNorm2d-40           [4, 128, 64, 64]             256\n",
      "           Conv2d-41           [4, 256, 64, 64]          33,024\n",
      "         DOConv2d-42           [4, 256, 64, 64]               0\n",
      "       SimpleGate-43           [4, 128, 64, 64]               0\n",
      "AdaptiveAvgPool2d-44             [4, 128, 1, 1]               0\n",
      "           Conv1d-45                [4, 128, 1]          81,920\n",
      "           Conv2d-46             [4, 128, 1, 1]          16,512\n",
      "         Identity-47             [4, 128, 1, 1]               0\n",
      "      LayerNorm2d-48           [4, 128, 64, 64]             256\n",
      "           Conv2d-49           [4, 256, 64, 64]          33,024\n",
      "       SimpleGate-50           [4, 128, 64, 64]               0\n",
      "           Conv2d-51           [4, 128, 64, 64]          16,512\n",
      "         Identity-52           [4, 128, 64, 64]               0\n",
      "         NAFBlock-53           [4, 128, 64, 64]               0\n",
      "         DOConv2d-54           [4, 256, 66, 35]               0\n",
      "             ReLU-55           [4, 256, 66, 35]               0\n",
      "         DOConv2d-56           [4, 256, 68, 37]               0\n",
      "         FFTBlock-57           [4, 128, 64, 64]               0\n",
      "           Conv2d-58           [4, 256, 32, 32]         131,328\n",
      "      LayerNorm2d-59           [4, 256, 32, 32]             512\n",
      "           Conv2d-60           [4, 512, 32, 32]         131,584\n",
      "         DOConv2d-61           [4, 512, 32, 32]               0\n",
      "       SimpleGate-62           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-63             [4, 256, 1, 1]               0\n",
      "           Conv1d-64                [4, 256, 1]         327,680\n",
      "           Conv2d-65             [4, 256, 1, 1]          65,792\n",
      "         Identity-66             [4, 256, 1, 1]               0\n",
      "      LayerNorm2d-67           [4, 256, 32, 32]             512\n",
      "           Conv2d-68           [4, 512, 32, 32]         131,584\n",
      "       SimpleGate-69           [4, 256, 32, 32]               0\n",
      "           Conv2d-70           [4, 256, 32, 32]          65,792\n",
      "         Identity-71           [4, 256, 32, 32]               0\n",
      "         NAFBlock-72           [4, 256, 32, 32]               0\n",
      "      LayerNorm2d-73           [4, 256, 32, 32]             512\n",
      "           Conv2d-74           [4, 512, 32, 32]         131,584\n",
      "         DOConv2d-75           [4, 512, 32, 32]               0\n",
      "       SimpleGate-76           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-77             [4, 256, 1, 1]               0\n",
      "           Conv1d-78                [4, 256, 1]         327,680\n",
      "           Conv2d-79             [4, 256, 1, 1]          65,792\n",
      "         Identity-80             [4, 256, 1, 1]               0\n",
      "      LayerNorm2d-81           [4, 256, 32, 32]             512\n",
      "           Conv2d-82           [4, 512, 32, 32]         131,584\n",
      "       SimpleGate-83           [4, 256, 32, 32]               0\n",
      "           Conv2d-84           [4, 256, 32, 32]          65,792\n",
      "         Identity-85           [4, 256, 32, 32]               0\n",
      "         NAFBlock-86           [4, 256, 32, 32]               0\n",
      "      LayerNorm2d-87           [4, 256, 32, 32]             512\n",
      "           Conv2d-88           [4, 512, 32, 32]         131,584\n",
      "         DOConv2d-89           [4, 512, 32, 32]               0\n",
      "       SimpleGate-90           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-91             [4, 256, 1, 1]               0\n",
      "           Conv1d-92                [4, 256, 1]         327,680\n",
      "           Conv2d-93             [4, 256, 1, 1]          65,792\n",
      "         Identity-94             [4, 256, 1, 1]               0\n",
      "      LayerNorm2d-95           [4, 256, 32, 32]             512\n",
      "           Conv2d-96           [4, 512, 32, 32]         131,584\n",
      "       SimpleGate-97           [4, 256, 32, 32]               0\n",
      "           Conv2d-98           [4, 256, 32, 32]          65,792\n",
      "         Identity-99           [4, 256, 32, 32]               0\n",
      "        NAFBlock-100           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-101           [4, 256, 32, 32]             512\n",
      "          Conv2d-102           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-103           [4, 512, 32, 32]               0\n",
      "      SimpleGate-104           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-105             [4, 256, 1, 1]               0\n",
      "          Conv1d-106                [4, 256, 1]         327,680\n",
      "          Conv2d-107             [4, 256, 1, 1]          65,792\n",
      "        Identity-108             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-109           [4, 256, 32, 32]             512\n",
      "          Conv2d-110           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-111           [4, 256, 32, 32]               0\n",
      "          Conv2d-112           [4, 256, 32, 32]          65,792\n",
      "        Identity-113           [4, 256, 32, 32]               0\n",
      "        NAFBlock-114           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-115           [4, 256, 32, 32]             512\n",
      "          Conv2d-116           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-117           [4, 512, 32, 32]               0\n",
      "      SimpleGate-118           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-119             [4, 256, 1, 1]               0\n",
      "          Conv1d-120                [4, 256, 1]         327,680\n",
      "          Conv2d-121             [4, 256, 1, 1]          65,792\n",
      "        Identity-122             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-123           [4, 256, 32, 32]             512\n",
      "          Conv2d-124           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-125           [4, 256, 32, 32]               0\n",
      "          Conv2d-126           [4, 256, 32, 32]          65,792\n",
      "        Identity-127           [4, 256, 32, 32]               0\n",
      "        NAFBlock-128           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-129           [4, 256, 32, 32]             512\n",
      "          Conv2d-130           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-131           [4, 512, 32, 32]               0\n",
      "      SimpleGate-132           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-133             [4, 256, 1, 1]               0\n",
      "          Conv1d-134                [4, 256, 1]         327,680\n",
      "          Conv2d-135             [4, 256, 1, 1]          65,792\n",
      "        Identity-136             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-137           [4, 256, 32, 32]             512\n",
      "          Conv2d-138           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-139           [4, 256, 32, 32]               0\n",
      "          Conv2d-140           [4, 256, 32, 32]          65,792\n",
      "        Identity-141           [4, 256, 32, 32]               0\n",
      "        NAFBlock-142           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-143           [4, 256, 32, 32]             512\n",
      "          Conv2d-144           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-145           [4, 512, 32, 32]               0\n",
      "      SimpleGate-146           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-147             [4, 256, 1, 1]               0\n",
      "          Conv1d-148                [4, 256, 1]         327,680\n",
      "          Conv2d-149             [4, 256, 1, 1]          65,792\n",
      "        Identity-150             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-151           [4, 256, 32, 32]             512\n",
      "          Conv2d-152           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-153           [4, 256, 32, 32]               0\n",
      "          Conv2d-154           [4, 256, 32, 32]          65,792\n",
      "        Identity-155           [4, 256, 32, 32]               0\n",
      "        NAFBlock-156           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-157           [4, 256, 32, 32]             512\n",
      "          Conv2d-158           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-159           [4, 512, 32, 32]               0\n",
      "      SimpleGate-160           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-161             [4, 256, 1, 1]               0\n",
      "          Conv1d-162                [4, 256, 1]         327,680\n",
      "          Conv2d-163             [4, 256, 1, 1]          65,792\n",
      "        Identity-164             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-165           [4, 256, 32, 32]             512\n",
      "          Conv2d-166           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-167           [4, 256, 32, 32]               0\n",
      "          Conv2d-168           [4, 256, 32, 32]          65,792\n",
      "        Identity-169           [4, 256, 32, 32]               0\n",
      "        NAFBlock-170           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-171           [4, 256, 32, 32]             512\n",
      "          Conv2d-172           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-173           [4, 512, 32, 32]               0\n",
      "      SimpleGate-174           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-175             [4, 256, 1, 1]               0\n",
      "          Conv1d-176                [4, 256, 1]         327,680\n",
      "          Conv2d-177             [4, 256, 1, 1]          65,792\n",
      "        Identity-178             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-179           [4, 256, 32, 32]             512\n",
      "          Conv2d-180           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-181           [4, 256, 32, 32]               0\n",
      "          Conv2d-182           [4, 256, 32, 32]          65,792\n",
      "        Identity-183           [4, 256, 32, 32]               0\n",
      "        NAFBlock-184           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-185           [4, 256, 32, 32]             512\n",
      "          Conv2d-186           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-187           [4, 512, 32, 32]               0\n",
      "      SimpleGate-188           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-189             [4, 256, 1, 1]               0\n",
      "          Conv1d-190                [4, 256, 1]         327,680\n",
      "          Conv2d-191             [4, 256, 1, 1]          65,792\n",
      "        Identity-192             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-193           [4, 256, 32, 32]             512\n",
      "          Conv2d-194           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-195           [4, 256, 32, 32]               0\n",
      "          Conv2d-196           [4, 256, 32, 32]          65,792\n",
      "        Identity-197           [4, 256, 32, 32]               0\n",
      "        NAFBlock-198           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-199           [4, 256, 32, 32]             512\n",
      "          Conv2d-200           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-201           [4, 512, 32, 32]               0\n",
      "      SimpleGate-202           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-203             [4, 256, 1, 1]               0\n",
      "          Conv1d-204                [4, 256, 1]         327,680\n",
      "          Conv2d-205             [4, 256, 1, 1]          65,792\n",
      "        Identity-206             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-207           [4, 256, 32, 32]             512\n",
      "          Conv2d-208           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-209           [4, 256, 32, 32]               0\n",
      "          Conv2d-210           [4, 256, 32, 32]          65,792\n",
      "        Identity-211           [4, 256, 32, 32]               0\n",
      "        NAFBlock-212           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-213           [4, 256, 32, 32]             512\n",
      "          Conv2d-214           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-215           [4, 512, 32, 32]               0\n",
      "      SimpleGate-216           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-217             [4, 256, 1, 1]               0\n",
      "          Conv1d-218                [4, 256, 1]         327,680\n",
      "          Conv2d-219             [4, 256, 1, 1]          65,792\n",
      "        Identity-220             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-221           [4, 256, 32, 32]             512\n",
      "          Conv2d-222           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-223           [4, 256, 32, 32]               0\n",
      "          Conv2d-224           [4, 256, 32, 32]          65,792\n",
      "        Identity-225           [4, 256, 32, 32]               0\n",
      "        NAFBlock-226           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-227           [4, 256, 32, 32]             512\n",
      "          Conv2d-228           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-229           [4, 512, 32, 32]               0\n",
      "      SimpleGate-230           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-231             [4, 256, 1, 1]               0\n",
      "          Conv1d-232                [4, 256, 1]         327,680\n",
      "          Conv2d-233             [4, 256, 1, 1]          65,792\n",
      "        Identity-234             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-235           [4, 256, 32, 32]             512\n",
      "          Conv2d-236           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-237           [4, 256, 32, 32]               0\n",
      "          Conv2d-238           [4, 256, 32, 32]          65,792\n",
      "        Identity-239           [4, 256, 32, 32]               0\n",
      "        NAFBlock-240           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-241           [4, 256, 32, 32]             512\n",
      "          Conv2d-242           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-243           [4, 512, 32, 32]               0\n",
      "      SimpleGate-244           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-245             [4, 256, 1, 1]               0\n",
      "          Conv1d-246                [4, 256, 1]         327,680\n",
      "          Conv2d-247             [4, 256, 1, 1]          65,792\n",
      "        Identity-248             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-249           [4, 256, 32, 32]             512\n",
      "          Conv2d-250           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-251           [4, 256, 32, 32]               0\n",
      "          Conv2d-252           [4, 256, 32, 32]          65,792\n",
      "        Identity-253           [4, 256, 32, 32]               0\n",
      "        NAFBlock-254           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-255           [4, 256, 32, 32]             512\n",
      "          Conv2d-256           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-257           [4, 512, 32, 32]               0\n",
      "      SimpleGate-258           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-259             [4, 256, 1, 1]               0\n",
      "          Conv1d-260                [4, 256, 1]         327,680\n",
      "          Conv2d-261             [4, 256, 1, 1]          65,792\n",
      "        Identity-262             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-263           [4, 256, 32, 32]             512\n",
      "          Conv2d-264           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-265           [4, 256, 32, 32]               0\n",
      "          Conv2d-266           [4, 256, 32, 32]          65,792\n",
      "        Identity-267           [4, 256, 32, 32]               0\n",
      "        NAFBlock-268           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-269           [4, 256, 32, 32]             512\n",
      "          Conv2d-270           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-271           [4, 512, 32, 32]               0\n",
      "      SimpleGate-272           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-273             [4, 256, 1, 1]               0\n",
      "          Conv1d-274                [4, 256, 1]         327,680\n",
      "          Conv2d-275             [4, 256, 1, 1]          65,792\n",
      "        Identity-276             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-277           [4, 256, 32, 32]             512\n",
      "          Conv2d-278           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-279           [4, 256, 32, 32]               0\n",
      "          Conv2d-280           [4, 256, 32, 32]          65,792\n",
      "        Identity-281           [4, 256, 32, 32]               0\n",
      "        NAFBlock-282           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-283           [4, 256, 32, 32]             512\n",
      "          Conv2d-284           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-285           [4, 512, 32, 32]               0\n",
      "      SimpleGate-286           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-287             [4, 256, 1, 1]               0\n",
      "          Conv1d-288                [4, 256, 1]         327,680\n",
      "          Conv2d-289             [4, 256, 1, 1]          65,792\n",
      "        Identity-290             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-291           [4, 256, 32, 32]             512\n",
      "          Conv2d-292           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-293           [4, 256, 32, 32]               0\n",
      "          Conv2d-294           [4, 256, 32, 32]          65,792\n",
      "        Identity-295           [4, 256, 32, 32]               0\n",
      "        NAFBlock-296           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-297           [4, 256, 32, 32]             512\n",
      "          Conv2d-298           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-299           [4, 512, 32, 32]               0\n",
      "      SimpleGate-300           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-301             [4, 256, 1, 1]               0\n",
      "          Conv1d-302                [4, 256, 1]         327,680\n",
      "          Conv2d-303             [4, 256, 1, 1]          65,792\n",
      "        Identity-304             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-305           [4, 256, 32, 32]             512\n",
      "          Conv2d-306           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-307           [4, 256, 32, 32]               0\n",
      "          Conv2d-308           [4, 256, 32, 32]          65,792\n",
      "        Identity-309           [4, 256, 32, 32]               0\n",
      "        NAFBlock-310           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-311           [4, 256, 32, 32]             512\n",
      "          Conv2d-312           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-313           [4, 512, 32, 32]               0\n",
      "      SimpleGate-314           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-315             [4, 256, 1, 1]               0\n",
      "          Conv1d-316                [4, 256, 1]         327,680\n",
      "          Conv2d-317             [4, 256, 1, 1]          65,792\n",
      "        Identity-318             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-319           [4, 256, 32, 32]             512\n",
      "          Conv2d-320           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-321           [4, 256, 32, 32]               0\n",
      "          Conv2d-322           [4, 256, 32, 32]          65,792\n",
      "        Identity-323           [4, 256, 32, 32]               0\n",
      "        NAFBlock-324           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-325           [4, 256, 32, 32]             512\n",
      "          Conv2d-326           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-327           [4, 512, 32, 32]               0\n",
      "      SimpleGate-328           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-329             [4, 256, 1, 1]               0\n",
      "          Conv1d-330                [4, 256, 1]         327,680\n",
      "          Conv2d-331             [4, 256, 1, 1]          65,792\n",
      "        Identity-332             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-333           [4, 256, 32, 32]             512\n",
      "          Conv2d-334           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-335           [4, 256, 32, 32]               0\n",
      "          Conv2d-336           [4, 256, 32, 32]          65,792\n",
      "        Identity-337           [4, 256, 32, 32]               0\n",
      "        NAFBlock-338           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-339           [4, 256, 32, 32]             512\n",
      "          Conv2d-340           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-341           [4, 512, 32, 32]               0\n",
      "      SimpleGate-342           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-343             [4, 256, 1, 1]               0\n",
      "          Conv1d-344                [4, 256, 1]         327,680\n",
      "          Conv2d-345             [4, 256, 1, 1]          65,792\n",
      "        Identity-346             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-347           [4, 256, 32, 32]             512\n",
      "          Conv2d-348           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-349           [4, 256, 32, 32]               0\n",
      "          Conv2d-350           [4, 256, 32, 32]          65,792\n",
      "        Identity-351           [4, 256, 32, 32]               0\n",
      "        NAFBlock-352           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-353           [4, 256, 32, 32]             512\n",
      "          Conv2d-354           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-355           [4, 512, 32, 32]               0\n",
      "      SimpleGate-356           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-357             [4, 256, 1, 1]               0\n",
      "          Conv1d-358                [4, 256, 1]         327,680\n",
      "          Conv2d-359             [4, 256, 1, 1]          65,792\n",
      "        Identity-360             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-361           [4, 256, 32, 32]             512\n",
      "          Conv2d-362           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-363           [4, 256, 32, 32]               0\n",
      "          Conv2d-364           [4, 256, 32, 32]          65,792\n",
      "        Identity-365           [4, 256, 32, 32]               0\n",
      "        NAFBlock-366           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-367           [4, 256, 32, 32]             512\n",
      "          Conv2d-368           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-369           [4, 512, 32, 32]               0\n",
      "      SimpleGate-370           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-371             [4, 256, 1, 1]               0\n",
      "          Conv1d-372                [4, 256, 1]         327,680\n",
      "          Conv2d-373             [4, 256, 1, 1]          65,792\n",
      "        Identity-374             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-375           [4, 256, 32, 32]             512\n",
      "          Conv2d-376           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-377           [4, 256, 32, 32]               0\n",
      "          Conv2d-378           [4, 256, 32, 32]          65,792\n",
      "        Identity-379           [4, 256, 32, 32]               0\n",
      "        NAFBlock-380           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-381           [4, 256, 32, 32]             512\n",
      "          Conv2d-382           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-383           [4, 512, 32, 32]               0\n",
      "      SimpleGate-384           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-385             [4, 256, 1, 1]               0\n",
      "          Conv1d-386                [4, 256, 1]         327,680\n",
      "          Conv2d-387             [4, 256, 1, 1]          65,792\n",
      "        Identity-388             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-389           [4, 256, 32, 32]             512\n",
      "          Conv2d-390           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-391           [4, 256, 32, 32]               0\n",
      "          Conv2d-392           [4, 256, 32, 32]          65,792\n",
      "        Identity-393           [4, 256, 32, 32]               0\n",
      "        NAFBlock-394           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-395           [4, 256, 32, 32]             512\n",
      "          Conv2d-396           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-397           [4, 512, 32, 32]               0\n",
      "      SimpleGate-398           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-399             [4, 256, 1, 1]               0\n",
      "          Conv1d-400                [4, 256, 1]         327,680\n",
      "          Conv2d-401             [4, 256, 1, 1]          65,792\n",
      "        Identity-402             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-403           [4, 256, 32, 32]             512\n",
      "          Conv2d-404           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-405           [4, 256, 32, 32]               0\n",
      "          Conv2d-406           [4, 256, 32, 32]          65,792\n",
      "        Identity-407           [4, 256, 32, 32]               0\n",
      "        NAFBlock-408           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-409           [4, 256, 32, 32]             512\n",
      "          Conv2d-410           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-411           [4, 512, 32, 32]               0\n",
      "      SimpleGate-412           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-413             [4, 256, 1, 1]               0\n",
      "          Conv1d-414                [4, 256, 1]         327,680\n",
      "          Conv2d-415             [4, 256, 1, 1]          65,792\n",
      "        Identity-416             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-417           [4, 256, 32, 32]             512\n",
      "          Conv2d-418           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-419           [4, 256, 32, 32]               0\n",
      "          Conv2d-420           [4, 256, 32, 32]          65,792\n",
      "        Identity-421           [4, 256, 32, 32]               0\n",
      "        NAFBlock-422           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-423           [4, 256, 32, 32]             512\n",
      "          Conv2d-424           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-425           [4, 512, 32, 32]               0\n",
      "      SimpleGate-426           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-427             [4, 256, 1, 1]               0\n",
      "          Conv1d-428                [4, 256, 1]         327,680\n",
      "          Conv2d-429             [4, 256, 1, 1]          65,792\n",
      "        Identity-430             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-431           [4, 256, 32, 32]             512\n",
      "          Conv2d-432           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-433           [4, 256, 32, 32]               0\n",
      "          Conv2d-434           [4, 256, 32, 32]          65,792\n",
      "        Identity-435           [4, 256, 32, 32]               0\n",
      "        NAFBlock-436           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-437           [4, 256, 32, 32]             512\n",
      "          Conv2d-438           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-439           [4, 512, 32, 32]               0\n",
      "      SimpleGate-440           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-441             [4, 256, 1, 1]               0\n",
      "          Conv1d-442                [4, 256, 1]         327,680\n",
      "          Conv2d-443             [4, 256, 1, 1]          65,792\n",
      "        Identity-444             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-445           [4, 256, 32, 32]             512\n",
      "          Conv2d-446           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-447           [4, 256, 32, 32]               0\n",
      "          Conv2d-448           [4, 256, 32, 32]          65,792\n",
      "        Identity-449           [4, 256, 32, 32]               0\n",
      "        NAFBlock-450           [4, 256, 32, 32]               0\n",
      "        DOConv2d-451           [4, 512, 34, 19]               0\n",
      "            ReLU-452           [4, 512, 34, 19]               0\n",
      "        DOConv2d-453           [4, 512, 36, 21]               0\n",
      "        FFTBlock-454           [4, 256, 32, 32]               0\n",
      "          Conv2d-455           [4, 512, 16, 16]         524,800\n",
      "     LayerNorm2d-456           [4, 512, 16, 16]           1,024\n",
      "          Conv2d-457          [4, 1024, 16, 16]         525,312\n",
      "        DOConv2d-458          [4, 1024, 16, 16]               0\n",
      "      SimpleGate-459           [4, 512, 16, 16]               0\n",
      "AdaptiveAvgPool2d-460             [4, 512, 1, 1]               0\n",
      "          Conv1d-461                [4, 512, 1]       1,310,720\n",
      "          Conv2d-462             [4, 512, 1, 1]         262,656\n",
      "        Identity-463             [4, 512, 1, 1]               0\n",
      "     LayerNorm2d-464           [4, 512, 16, 16]           1,024\n",
      "          Conv2d-465          [4, 1024, 16, 16]         525,312\n",
      "      SimpleGate-466           [4, 512, 16, 16]               0\n",
      "          Conv2d-467           [4, 512, 16, 16]         262,656\n",
      "        Identity-468           [4, 512, 16, 16]               0\n",
      "        NAFBlock-469           [4, 512, 16, 16]               0\n",
      "          Conv2d-470          [4, 1024, 16, 16]         524,288\n",
      "    PixelShuffle-471           [4, 256, 32, 32]               0\n",
      "     LayerNorm2d-472           [4, 256, 32, 32]             512\n",
      "          Conv2d-473           [4, 512, 32, 32]         131,584\n",
      "        DOConv2d-474           [4, 512, 32, 32]               0\n",
      "      SimpleGate-475           [4, 256, 32, 32]               0\n",
      "AdaptiveAvgPool2d-476             [4, 256, 1, 1]               0\n",
      "          Conv1d-477                [4, 256, 1]         327,680\n",
      "          Conv2d-478             [4, 256, 1, 1]          65,792\n",
      "        Identity-479             [4, 256, 1, 1]               0\n",
      "     LayerNorm2d-480           [4, 256, 32, 32]             512\n",
      "          Conv2d-481           [4, 512, 32, 32]         131,584\n",
      "      SimpleGate-482           [4, 256, 32, 32]               0\n",
      "          Conv2d-483           [4, 256, 32, 32]          65,792\n",
      "        Identity-484           [4, 256, 32, 32]               0\n",
      "        NAFBlock-485           [4, 256, 32, 32]               0\n",
      "          Conv2d-486           [4, 512, 32, 32]         131,072\n",
      "    PixelShuffle-487           [4, 128, 64, 64]               0\n",
      "     LayerNorm2d-488           [4, 128, 64, 64]             256\n",
      "          Conv2d-489           [4, 256, 64, 64]          33,024\n",
      "        DOConv2d-490           [4, 256, 64, 64]               0\n",
      "      SimpleGate-491           [4, 128, 64, 64]               0\n",
      "AdaptiveAvgPool2d-492             [4, 128, 1, 1]               0\n",
      "          Conv1d-493                [4, 128, 1]          81,920\n",
      "          Conv2d-494             [4, 128, 1, 1]          16,512\n",
      "        Identity-495             [4, 128, 1, 1]               0\n",
      "     LayerNorm2d-496           [4, 128, 64, 64]             256\n",
      "          Conv2d-497           [4, 256, 64, 64]          33,024\n",
      "      SimpleGate-498           [4, 128, 64, 64]               0\n",
      "          Conv2d-499           [4, 128, 64, 64]          16,512\n",
      "        Identity-500           [4, 128, 64, 64]               0\n",
      "        NAFBlock-501           [4, 128, 64, 64]               0\n",
      "          Conv2d-502           [4, 256, 64, 64]          32,768\n",
      "    PixelShuffle-503          [4, 64, 128, 128]               0\n",
      "     LayerNorm2d-504          [4, 64, 128, 128]             128\n",
      "          Conv2d-505         [4, 128, 128, 128]           8,320\n",
      "        DOConv2d-506         [4, 128, 128, 128]               0\n",
      "      SimpleGate-507          [4, 64, 128, 128]               0\n",
      "AdaptiveAvgPool2d-508              [4, 64, 1, 1]               0\n",
      "          Conv1d-509                 [4, 64, 1]          12,288\n",
      "          Conv2d-510              [4, 64, 1, 1]           4,160\n",
      "        Identity-511              [4, 64, 1, 1]               0\n",
      "     LayerNorm2d-512          [4, 64, 128, 128]             128\n",
      "          Conv2d-513         [4, 128, 128, 128]           8,320\n",
      "      SimpleGate-514          [4, 64, 128, 128]               0\n",
      "          Conv2d-515          [4, 64, 128, 128]           4,160\n",
      "        Identity-516          [4, 64, 128, 128]               0\n",
      "        NAFBlock-517          [4, 64, 128, 128]               0\n",
      "          Conv2d-518         [4, 128, 128, 128]           8,192\n",
      "    PixelShuffle-519          [4, 32, 256, 256]               0\n",
      "     LayerNorm2d-520          [4, 32, 256, 256]              64\n",
      "          Conv2d-521          [4, 64, 256, 256]           2,112\n",
      "        DOConv2d-522          [4, 64, 256, 256]               0\n",
      "      SimpleGate-523          [4, 32, 256, 256]               0\n",
      "AdaptiveAvgPool2d-524              [4, 32, 1, 1]               0\n",
      "          Conv1d-525                 [4, 32, 1]           3,072\n",
      "          Conv2d-526              [4, 32, 1, 1]           1,056\n",
      "        Identity-527              [4, 32, 1, 1]               0\n",
      "     LayerNorm2d-528          [4, 32, 256, 256]              64\n",
      "          Conv2d-529          [4, 64, 256, 256]           2,112\n",
      "      SimpleGate-530          [4, 32, 256, 256]               0\n",
      "          Conv2d-531          [4, 32, 256, 256]           1,056\n",
      "        Identity-532          [4, 32, 256, 256]               0\n",
      "        NAFBlock-533          [4, 32, 256, 256]               0\n",
      "          Conv2d-534           [4, 3, 256, 256]             867\n",
      "================================================================\n",
      "Total params: 25,721,379\n",
      "Trainable params: 25,721,379\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 6861.96\n",
      "Params size (MB): 98.12\n",
      "Estimated Total Size (MB): 6963.08\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# # Dataset check\n",
    "# lq, gt = next(iter(train_dl))\n",
    "# img_sample = torch.cat((lq, gt), -2) # 높이(height)를 기준으로 이미지를 연결하기\n",
    "# save_image(img_sample, \"train_dl_img.png\", nrow=batch_size, normalize=False)\n",
    "\n",
    "# Network check\n",
    "from torchsummary import summary\n",
    "summary(net,(3,256,256),batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: curr_epoch = list(map(lambda x : x[-8:-4], glob(f'd:/checkpoint/{loss}/*')))[-1]\n",
    "except: curr_epoch = '0000'; resume_train = False\n",
    "\n",
    "if resume_train:\n",
    "    state_dict = torch.load(f'd:/checkpoint/{loss}/checkpoint.{curr_epoch}.pth')\n",
    "\n",
    "    resume_epoch = state_dict['epoch'] + 1  # +1 부터 시작할 수 있게\n",
    "    optimizer_state_dict = state_dict['optimizer_state_dict']\n",
    "    model_state_dict = state_dict['model_state_dict']\n",
    "    scheduler_state_dict = state_dict['scheduler_state_dict']\n",
    "\n",
    "    print(f'resume_epoch: {resume_epoch}')\n",
    "\n",
    "    optimizer_g.load_state_dict(optimizer_state_dict)\n",
    "    net.load_state_dict(model_state_dict, strict=True)\n",
    "    scheduler.load_state_dict(scheduler_state_dict)\n",
    "else:\n",
    "    resume_epoch = 0\n",
    "\n",
    "for epoch in range(resume_epoch, epochs):\n",
    "    batch_cnt = 0\n",
    "    pbar = tqdm(train_dl)\n",
    "    for i, batch in enumerate(pbar):\n",
    "        batch_cnt += 1\n",
    "\n",
    "        lq, gt = batch\n",
    "        lq = lq.to(device)\n",
    "        gt = gt.to(device)\n",
    "        \n",
    "        optimizer_g.zero_grad()\n",
    "\n",
    "        preds_t = net(lq)\n",
    "        if not isinstance(preds_t, list): # preds가 tensor라서 이걸 list로 바꿔주는거\n",
    "            preds = [preds_t]\n",
    "\n",
    "        output = preds[-1]\n",
    "\n",
    "        # pixel loss\n",
    "        l_total = 0.\n",
    "        l_pix = 0.\n",
    "        l_pix_fft = 0.\n",
    "        for pred in preds:\n",
    "            l_pix += cri_pix(pred, gt)   # 누적연산은 좋지않댔는데, float를 붙이면 해결가능함 (https://pytorch.org/docs/stable/notes/faq.html#my-model-reports-cuda-runtime-error-2-out-of-memory)\n",
    "            #----------------------------------------------------------#\n",
    "            l_pix_fft += cri_pix_fft(pred, gt)\n",
    "            #----------------------------------------------------------#\n",
    "        \n",
    "        l_total += ((0.05 * l_pix_fft) + l_pix)\n",
    "\n",
    "        l_total = l_total + 0. * sum(p.sum() for p in net.parameters())\n",
    "        pbar.set_postfix_str(f\"epoch: {epoch}, l_pix: {l_pix}, l_pix_fft: {0.05 * l_pix_fft}, l_total: {l_total} lr: {optimizer_g.param_groups[0]['lr']}\")\n",
    "\n",
    "\n",
    "        l_total.backward()\n",
    "        use_grad_clip = True\n",
    "        if use_grad_clip:\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 0.01)\n",
    "        optimizer_g.step()\n",
    "\n",
    "        if batch_cnt % 300 == 0:\n",
    "            img_sample = torch.cat((lq.data, preds_t, gt.data), -2) # 높이(height)를 기준으로 이미지를 연결하기\n",
    "            save_image(img_sample, f\"d:/img/result/{loss}/result{epoch:04d}_{batch_cnt}.png\", nrow=batch_size, normalize=False)    \n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    img_sample = torch.cat((lq.data, preds_t, gt.data), -2) # 높이(height)를 기준으로 이미지를 연결하기\n",
    "    save_image(img_sample, f\"d:/img/result/{loss}/result{epoch:04d}.png\", nrow=batch_size, normalize=False)\n",
    "    print(f\"[Sample img 저장 완료] d:/img/result/{loss}/result{epoch:04d}.png\")\n",
    "    \n",
    "    if epoch % 3 == 0:\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'optimizer_state_dict': optimizer_g.state_dict(),\n",
    "                'scheduler_state_dict':scheduler.state_dict(),\n",
    "                'model_state_dict': net.state_dict(),\n",
    "            }, f'd:/checkpoint/{loss}/checkpoint.{epoch:04d}.pth')\n",
    "        print(f'[Check point 저장 완료] d:/checkpoint/{loss}/checkpoint.{epoch:04d}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint0230를 이용하여 Inference 진행\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    isinstance(net, NAFFTNet)\n",
    "except:\n",
    "    net = NAFFTNet(img_channel=img_channel, width=width, middle_blk_num=middle_blk_num,\n",
    "                    enc_blk_nums=enc_blks, dec_blk_nums=dec_blks)\n",
    "    net.to(device)\n",
    "\n",
    "try: curr_epoch = list(map(lambda x : x[-8:-4], glob(f'd:/checkpoint/{loss}/*')))[-1]; \n",
    "except: raise Exception(f'd:/checkpoint/{loss}/ 에 체크포인트가 없습니다.')\n",
    "# curr_epoch = '0135'\n",
    "print(f'checkpoint{curr_epoch}를 이용하여 Inference 진행')\n",
    "state_dict = torch.load(f'd:/checkpoint/{loss}/checkpoint.{curr_epoch}.pth')\n",
    "model_state_dict = state_dict['model_state_dict']\n",
    "net.load_state_dict(model_state_dict, strict=True) \n",
    "net.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 이미지 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lq, gt = next(iter(test_dl))\n",
    "# lq = lq.to(device)\n",
    "# gt = gt.to(device)\n",
    "# img_res = net(lq)\n",
    "\n",
    "# img_sample = torch.cat((lq.data, img_res.data, gt.data), -2) # 높이(height)를 기준으로 이미지를 연결하기\n",
    "# save_image(img_sample, f\"img/result/{loss}/result{curr_epoch}_test.png\", nrow=batch_size, normalize=False)\n",
    "# print(f\"[저장 완료] img/result/{loss}/result{curr_epoch}_test.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSNR 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.psnr_measure import psnr\n",
    "import time\n",
    "psnr_avg = 0\n",
    "sum_psnr = 0\n",
    "curr_psnr = 0\n",
    "pbar = tqdm(test_dl)\n",
    "for i, batch in enumerate(pbar):\n",
    "    if i == 0: continue\n",
    "    # pbar.set_postfix_str(f\"current psnr_avg: {psnr_avg}\")\n",
    "    # pbar.set_postfix_str(f\"current psnr_avg: {psnr_avg}\")\n",
    "    \n",
    "    lq, gt = batch\n",
    "    lq = lq.to(device)\n",
    "    gt = gt.to(device)\n",
    "    results = net(lq)\n",
    "    for j, result in enumerate(results):\n",
    "        # curr_psnr = psnr(result, gt[j])\n",
    "        # img_sample = torch.cat((result.data, gt[j].data), -2) # 높이(height)를 기준으로 이미지를 연결하기\n",
    "        save_image(result.data, f\"tmp3/{test_batch_size*i+j}.png\", nrow=batch_size, normalize=False)\n",
    "    \n",
    "        # sum_psnr += curr_psnr\n",
    "        # psnr_avg = sum_psnr / (test_batch_size*i + j)\n",
    "        # print(curr_psnr)\n",
    "    # for result in results:\n",
    "        # sum_psnr += psnr(result, gt)\n",
    "    # psnr_avg = sum_psnr / (i * batch_size)\n",
    "\n",
    "# print(psnr_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference With CPU\n",
    "demo_image = transforms.ToTensor()(Image.open('./blurry.png').convert(\"RGB\")).unsqueeze(dim=0)\n",
    "net.to('cpu')\n",
    "img_res = net(demo_image)\n",
    "save_image(img_res, f\"blurry_result_{loss}_{curr_epoch}.png\", nrow=batch_size, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('yolo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff0323b92811708b19364feae987dc3a6297456b8196963308283135522dbebe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
