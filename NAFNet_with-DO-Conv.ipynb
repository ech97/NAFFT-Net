{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 주석으로 설명 달아놓음\n",
    "# with FFT  - DO-Conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOSS가 많으면\n",
    "Loss가 converge하는지 봐야해 (네트워크한테 이래라 저래라하는거니깐 수렴이 어렵지)\n",
    "각각의 Loss를 어떻게 섞어야하는지 (감으로! 이론X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 관리\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# 이미지 처리\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# For Custom Dataset\n",
    "import torchvision.transforms.functional as TVF\n",
    "\n",
    "# Network\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# For Custom Network\n",
    "import math as m\n",
    "\n",
    "# Utils\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO-Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from torch._jit_internal import Optional\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "class DOConv2d(Module):\n",
    "    \"\"\"\n",
    "       DOConv2d can be used as an alternative for torch.nn.Conv2d.\n",
    "       The interface is similar to that of Conv2d, with one exception:\n",
    "            1. D_mul: the depth multiplier for the over-parameterization.\n",
    "       Note that the groups parameter switchs between DO-Conv (groups=1),\n",
    "       DO-DConv (groups=in_channels), DO-GConv (otherwise).\n",
    "    \"\"\"\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
    "                     'padding_mode', 'output_padding', 'in_channels',\n",
    "                     'out_channels', 'kernel_size', 'D_mul']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, D_mul=None, stride=1,\n",
    "                 padding=1, dilation=1, groups=1, bias=False, padding_mode='zeros', simam=False):\n",
    "        super(DOConv2d, self).__init__()\n",
    "\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "        stride = (stride, stride)\n",
    "        padding = (padding, padding)\n",
    "        dilation = (dilation, dilation)\n",
    "\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        self._padding_repeated_twice = tuple(x for x in self.padding for _ in range(2))\n",
    "        self.simam = simam\n",
    "        #################################### Initailization of D & W ###################################\n",
    "        M = self.kernel_size[0]\n",
    "        N = self.kernel_size[1]\n",
    "        self.D_mul = M * N if D_mul is None or M * N <= 1 else D_mul\n",
    "        self.W = Parameter(torch.Tensor(out_channels, in_channels // groups, self.D_mul))\n",
    "        init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "        if M * N > 1:\n",
    "            self.D = Parameter(torch.Tensor(in_channels, M * N, self.D_mul))\n",
    "            init_zero = np.zeros([in_channels, M * N, self.D_mul], dtype=np.float32)\n",
    "            self.D.data = torch.from_numpy(init_zero)\n",
    "\n",
    "            eye = torch.reshape(torch.eye(M * N, dtype=torch.float32), (1, M * N, M * N))\n",
    "            D_diag = eye.repeat((in_channels, 1, self.D_mul // (M * N)))\n",
    "            if self.D_mul % (M * N) != 0:  # the cases when D_mul > M * N\n",
    "                zeros = torch.zeros([in_channels, M * N, self.D_mul % (M * N)])\n",
    "                self.D_diag = Parameter(torch.cat([D_diag, zeros], dim=2), requires_grad=False)\n",
    "            else:  # the case when D_mul = M * N\n",
    "                self.D_diag = Parameter(D_diag, requires_grad=False)\n",
    "        ##################################################################################################\n",
    "        if simam:\n",
    "            self.simam_block = simam_module()\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.W)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(DOConv2d, self).__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "\n",
    "    def _conv_forward(self, input, weight):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, self.bias, self.stride,\n",
    "                            (0, 0), self.dilation, self.groups)\n",
    "        return F.conv2d(input, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input):\n",
    "        M = self.kernel_size[0]\n",
    "        N = self.kernel_size[1]\n",
    "        DoW_shape = (self.out_channels, self.in_channels // self.groups, M, N)\n",
    "        if M * N > 1:\n",
    "            ######################### Compute DoW #################\n",
    "            # (input_channels, D_mul, M * N)\n",
    "            D = self.D + self.D_diag\n",
    "            W = torch.reshape(self.W, (self.out_channels // self.groups, self.in_channels, self.D_mul))\n",
    "\n",
    "            # einsum outputs (out_channels // groups, in_channels, M * N),\n",
    "            # which is reshaped to\n",
    "            # (out_channels, in_channels // groups, M, N)\n",
    "            DoW = torch.reshape(torch.einsum('ims,ois->oim', D, W), DoW_shape)\n",
    "            #######################################################\n",
    "        else:\n",
    "            DoW = torch.reshape(self.W, DoW_shape)\n",
    "        if self.simam:\n",
    "            DoW_h1, DoW_h2 = torch.chunk(DoW, 2, dim=2)\n",
    "            DoW = torch.cat([self.simam_block(DoW_h1), DoW_h2], dim=2)\n",
    "\n",
    "        return self._conv_forward(input, DoW)\n",
    "class DOConv2d_eval(Module):\n",
    "    \"\"\"\n",
    "       DOConv2d can be used as an alternative for torch.nn.Conv2d.\n",
    "       The interface is similar to that of Conv2d, with one exception:\n",
    "            1. D_mul: the depth multiplier for the over-parameterization.\n",
    "       Note that the groups parameter switchs between DO-Conv (groups=1),\n",
    "       DO-DConv (groups=in_channels), DO-GConv (otherwise).\n",
    "    \"\"\"\n",
    "    __constants__ = ['stride', 'padding', 'dilation', 'groups',\n",
    "                     'padding_mode', 'output_padding', 'in_channels',\n",
    "                     'out_channels', 'kernel_size', 'D_mul']\n",
    "    __annotations__ = {'bias': Optional[torch.Tensor]}\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, D_mul=None, stride=1,\n",
    "                 padding=1, dilation=1, groups=1, bias=False, padding_mode='zeros', simam=False):\n",
    "        super(DOConv2d_eval, self).__init__()\n",
    "\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "        stride = (stride, stride)\n",
    "        padding = (padding, padding)\n",
    "        dilation = (dilation, dilation)\n",
    "\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        valid_padding_modes = {'zeros', 'reflect', 'replicate', 'circular'}\n",
    "        if padding_mode not in valid_padding_modes:\n",
    "            raise ValueError(\"padding_mode must be one of {}, but got padding_mode='{}'\".format(\n",
    "                valid_padding_modes, padding_mode))\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        self._padding_repeated_twice = tuple(x for x in self.padding for _ in range(2))\n",
    "        self.simam = simam\n",
    "        #################################### Initailization of D & W ###################################\n",
    "        M = self.kernel_size[0]\n",
    "        N = self.kernel_size[1]\n",
    "        self.W = Parameter(torch.Tensor(out_channels, in_channels // groups, M, N))\n",
    "        init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "        self.register_parameter('bias', None)\n",
    "    def extra_repr(self):\n",
    "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
    "             ', stride={stride}')\n",
    "        if self.padding != (0,) * len(self.padding):\n",
    "            s += ', padding={padding}'\n",
    "        if self.dilation != (1,) * len(self.dilation):\n",
    "            s += ', dilation={dilation}'\n",
    "        if self.groups != 1:\n",
    "            s += ', groups={groups}'\n",
    "        if self.bias is None:\n",
    "            s += ', bias=False'\n",
    "        if self.padding_mode != 'zeros':\n",
    "            s += ', padding_mode={padding_mode}'\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(DOConv2d, self).__setstate__(state)\n",
    "        if not hasattr(self, 'padding_mode'):\n",
    "            self.padding_mode = 'zeros'\n",
    "\n",
    "    def _conv_forward(self, input, weight):\n",
    "        if self.padding_mode != 'zeros':\n",
    "            return F.conv2d(F.pad(input, self._padding_repeated_twice, mode=self.padding_mode),\n",
    "                            weight, self.bias, self.stride,\n",
    "                            (0, 0), self.dilation, self.groups)\n",
    "        return F.conv2d(input, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self._conv_forward(input, self.W)\n",
    "\n",
    "class simam_module(torch.nn.Module):\n",
    "    def __init__(self, e_lambda=1e-4):\n",
    "        super(simam_module, self).__init__()\n",
    "        self.activaton = nn.Sigmoid()\n",
    "        self.e_lambda = e_lambda\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        n = w * h - 1\n",
    "        x_minus_mu_square = (x - x.mean(dim=[2, 3], keepdim=True)).pow(2)\n",
    "        y = x_minus_mu_square / (4 * (x_minus_mu_square.sum(dim=[2, 3], keepdim=True) / n + self.e_lambda)) + 0.5\n",
    "        return x * self.activaton(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LayerNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormFunction(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight, bias, eps):\n",
    "        ctx.eps = eps\n",
    "        N, C, H, W = x.size()\n",
    "        mu = x.mean(1, keepdim=True)\n",
    "        var = (x - mu).pow(2).mean(1, keepdim=True)\n",
    "        y = (x - mu) / (var + eps).sqrt()\n",
    "        ctx.save_for_backward(y, var, weight)\n",
    "        y = weight.view(1, C, 1, 1) * y + bias.view(1, C, 1, 1)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        eps = ctx.eps\n",
    "\n",
    "        N, C, H, W = grad_output.size()\n",
    "        y, var, weight = ctx.saved_tensors\n",
    "        g = grad_output * weight.view(1, C, 1, 1)\n",
    "        mean_g = g.mean(dim=1, keepdim=True)\n",
    "\n",
    "        mean_gy = (g * y).mean(dim=1, keepdim=True)\n",
    "        gx = 1. / torch.sqrt(var + eps) * (g - y * mean_gy - mean_g)\n",
    "        return gx, (grad_output * y).sum(dim=3).sum(dim=2).sum(dim=0), grad_output.sum(dim=3).sum(dim=2).sum(\n",
    "            dim=0), None\n",
    "\n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, channels, eps=1e-6):\n",
    "        super(LayerNorm2d, self).__init__()\n",
    "        self.register_parameter('weight', nn.Parameter(torch.ones(channels)))\n",
    "        self.register_parameter('bias', nn.Parameter(torch.zeros(channels)))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return LayerNormFunction.apply(x, self.weight, self.bias, self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAFNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTBlock(nn.Module):\n",
    "    def __init__(self, out_channel, norm='backward'):\n",
    "        super(FFTBlock, self).__init__()\n",
    "        self.main_fft = nn.Sequential(\n",
    "            DOConv2d(out_channel*2, out_channel*2, kernel_size=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            DOConv2d(out_channel*2, out_channel*2, kernel_size=1, stride=1)\n",
    "        )\n",
    "        self.norm = norm\n",
    "    def forward(self, x):\n",
    "        _, C, H, W = x.shape\n",
    "        dim = 1\n",
    "        y = torch.fft.rfft2(x, norm=self.norm)\n",
    "        y_imag = y.imag\n",
    "        y_real = y.real\n",
    "        y_f = torch.cat([y_real, y_imag], dim=dim)\n",
    "        \n",
    "        y = self.main_fft(y_f)\n",
    "        \n",
    "        y_real, y_imag = torch.chunk(y, 2, dim=dim)\n",
    "        y = torch.complex(y_real, y_imag)\n",
    "        y = torch.fft.irfft2(y, s=(H, W), norm=self.norm)\n",
    "        return x + y    # 입력으로 들어온 값과, FFT층 통과한 이미지 더해서 return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Channel 방향으로 쪼개서 Element-wise mul 진행\n",
    "class SimpleGate(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x1, x2 = x.chunk(2, dim=1)  # (batch, channel, row, col)\n",
    "        return x1 * x2\n",
    "\n",
    "class NAFBlock(nn.Module):\n",
    "    def __init__(self, c, DW_Expand=2, FFN_Expand=2, drop_out_rate=0.):\n",
    "        super(NAFBlock, self).__init__()\n",
    "        dw_channel = c * DW_Expand  # Embedding 과정과, Non-linear 과정에서 정보유실을 방지하기위해 Expansion함 // 여기서는 SCA(채널쪼개서 서로 곱하는)부분에서 발생하는 non-linearity에서의 정보유실 방지\n",
    "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=dw_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
    "        self.conv2 = DOConv2d(in_channels=dw_channel, out_channels=dw_channel, kernel_size=3, padding=1, stride=1, groups=dw_channel,\n",
    "                               bias=True)\n",
    "        self.conv3 = nn.Conv2d(in_channels=dw_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
    "        \n",
    "        # Simplified Channel Attention\n",
    "        # GAP진행해서 Channel별로 정보 뽑고, Point-wise Conv 진행후에, 원래 feature와 channel-wise multiplication 진행\n",
    "        self.sca = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels=dw_channel // 2, out_channels=dw_channel // 2, kernel_size=1, padding=0, stride=1,\n",
    "                      groups=1, bias=True),\n",
    "        )\n",
    "        \n",
    "        # 2@@@추가한부분, MLP인 1x1 point conv를 1d-conv로 교체했음. // 매우 빠르다는 장점도 있고, ECA에서 성능향상도 있었음. 기존의 SE Block을 대체하기 위해 나온 개념.\n",
    "        #---------------------------------------------------#\n",
    "        t = int(abs((m.log(dw_channel//2, 2) + 1) / 2))\n",
    "        self.k_size = t if t % 2 else t + 1 # k_size가 홀수여야 padding이 가능함.\n",
    "        self.sca1 = nn.AdaptiveAvgPool2d(1)\n",
    "        self.sca2 = nn.Conv1d(in_channels=dw_channel // 2, out_channels=dw_channel // 2, kernel_size = self.k_size, padding=(self.k_size-1)//2, bias=False)   #@@@ 추가한 부분\n",
    "        #---------------------------------------------------#\n",
    "\n",
    "        # SimpleGate\n",
    "        self.sg = SimpleGate()\n",
    "\n",
    "        ffn_channel = FFN_Expand * c\n",
    "        self.conv4 = nn.Conv2d(in_channels=c, out_channels=ffn_channel, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
    "        self.conv5 = nn.Conv2d(in_channels=ffn_channel // 2, out_channels=c, kernel_size=1, padding=0, stride=1, groups=1, bias=True)\n",
    "\n",
    "        self.norm1 = LayerNorm2d(c, eps=1e-6)\n",
    "        self.norm2 = LayerNorm2d(c, eps=1e-6)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
    "        self.dropout2 = nn.Dropout(drop_out_rate) if drop_out_rate > 0. else nn.Identity()\n",
    "\n",
    "        # Normalization 파라미터, 단순히 정규화해서 ReLU를 통과시키면, 대부분의 Param이 소멸되므로, beta와 같은 bias 추가\n",
    "        self.beta = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
    "        self.gamma = nn.Parameter(torch.zeros((1, c, 1, 1)), requires_grad=True)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = inp\n",
    "\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x = self.conv1(x)   # Expansion (point-wise conv)\n",
    "        x = self.conv2(x)   # Depthwise Conv\n",
    "        x = self.sg(x)      # 채널 쪼개서 곱하기 for non-linearity / 이 과정에서 Channel수가 반으로 줄음\n",
    "        # x = x * self.sca(x) # Channel-wise Attension (element-wise mult)\n",
    "        # 2@@@추가한부분, MLP인 1x1 point conv를 1d-conv로 교체했음. // 매우 빠르다는 장점도 있고, ECA에서 성능향상도 있었음. 기존의 SE Block을 대체하기 위해 나온 개념.\n",
    "        #---------------------------------------------------#\n",
    "        x = self.sca1(x)\n",
    "        x = self.sca2(x.squeeze(-1)).unsqueeze(-1)  # conv1d는 [Batch_N, Channel, Length] 형식으로 받기때문에 기존의 [Batch_N, Channel, H, W]를 수정해줘야함\n",
    "        #---------------------------------------------------#\n",
    "        x = self.conv3(x)   # 원래 크기로 복구 (point-wise conv)\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        y = inp + x * self.beta\n",
    "\n",
    "        x = self.conv4(self.norm2(y))   # 채널 수 ffn_channel으로 뻥튀기 (point-wise conv)\n",
    "        x = self.sg(x)      # 채널 수 반(ffn_channel // 2)으로 줄음\n",
    "        x = self.conv5(x)   # 원래 채널 수로 복구 (point-wise conv)\n",
    "\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        return y + x * self.gamma\n",
    "\n",
    "class NAFNet(nn.Module):\n",
    "\n",
    "    def __init__(self, img_channel=3, width=32, middle_blk_num=1, enc_blk_nums=[], dec_blk_nums=[]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.intro = nn.Conv2d(in_channels=img_channel, out_channels=width, kernel_size=3, padding=1, stride=1, groups=1,\n",
    "                              bias=True)\n",
    "        self.ending = nn.Conv2d(in_channels=width, out_channels=img_channel, kernel_size=3, padding=1, stride=1, groups=1,\n",
    "                              bias=True)    # 최종적으로는 채널 3으로 나가야하니깐.\n",
    "\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.decoders = nn.ModuleList()\n",
    "        self.middle_blks = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        #------------------------------------------------------------------#\n",
    "        self.fftBlock = nn.ModuleList() \n",
    "        #------------------------------------------------------------------#\n",
    "\n",
    "        chan = width\n",
    "        for num in enc_blk_nums:    # [1, 1, 1, 28]\n",
    "            self.encoders.append(\n",
    "                nn.Sequential(\n",
    "                    *[NAFBlock(chan) for _ in range(num)]   # 변수 num(1~28)이 여기에만 붙어있어. 즉 얘는 28번 반복. 하지만 down은 len(list) == 4번만 진행\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            self.fftBlock.append(\n",
    "                nn.Sequential(\n",
    "                    FFTBlock(chan)\n",
    "                )\n",
    "            )\n",
    "            self.downs.append(      # len(enc_blk_nums) == 4, 즉 2**4 배 만큼 이미지크기 감소\n",
    "                nn.Conv2d(chan, 2*chan, kernel_size=2, stride=2)    # 이미지 크기는 절반 / 채널은 2 배\n",
    "            )\n",
    "            chan = chan * 2\n",
    "\n",
    "        self.middle_blks = \\\n",
    "            nn.Sequential(\n",
    "                *[NAFBlock(chan) for _ in range(middle_blk_num)]\n",
    "            )\n",
    "\n",
    "        for num in dec_blk_nums:\n",
    "            self.ups.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(chan, chan*2, kernel_size=1, bias=False),   # point-wise conv. 채널 수 2배 뻥튀기\n",
    "                    \n",
    "                    nn.PixelShuffle(upscale_factor=2)   # Feature map의 수많은 channel을 이용하여, pixel 위치에 맞는 각 Channel의 값을 떼어와서 feature map 확장  # 논문 참고 (https://mole-starseeker.tistory.com/m/84)\n",
    "                                                        # 이미지를 가로 세로 2배씩 확장한다면, 채널은 4개가 필요함 // 따라서 채널이 upscale_factor ** 2 만큼 감소\n",
    "                )\n",
    "            )\n",
    "            chan = chan // 2    # 채널 수 다시 2배 감소\n",
    "            self.decoders.append(\n",
    "                nn.Sequential(\n",
    "                    *[NAFBlock(chan) for _ in range(num)]\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "        self.padder_size = 2 ** len(self.encoders)  # == len(enc_blk_nums) == 4, down으로 인해 감소된 배수\n",
    "\n",
    "    def forward(self, inp):\n",
    "        B, C, H, W = inp.shape\n",
    "        inp = self.check_image_size(inp)    # padding 안함\n",
    "\n",
    "        x = self.intro(inp)\n",
    "\n",
    "        encs = []\n",
    "\n",
    "\n",
    "        # [1, 1, 1, 28] 이 었으니, 4번만큼 반복\n",
    "        for encoder, fftBlock, down in zip(self.encoders, self.fftBlock, self.downs):\n",
    "            x = encoder(x)\n",
    "            encs.append(fftBlock(x))\n",
    "            x = down(x)\n",
    "\n",
    "        x = self.middle_blks(x)\n",
    "\n",
    "        for decoder, up, enc_skip in zip(self.decoders, self.ups, encs[::-1]):\n",
    "            x = up(x)           # enc와 채널개수 및 이미지 크기를 맞춤\n",
    "            x = x + enc_skip    # skip-connection 이용\n",
    "            x = decoder(x)      # \n",
    "\n",
    "        x = self.ending(x)\n",
    "        x = x + inp\n",
    "\n",
    "        return x[:, :, :H, :W]\n",
    "\n",
    "    def check_image_size(self, x):\n",
    "        _, _, h, w = x.size()\n",
    "        mod_pad_h = (self.padder_size - h % self.padder_size) % self.padder_size\n",
    "        mod_pad_w = (self.padder_size - w % self.padder_size) % self.padder_size\n",
    "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRandomCrop(nn.Module):\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_params(img, output_size=(256, 256)):\n",
    "        w, h = TVF._get_image_size(img)\n",
    "        th, tw = output_size\n",
    "\n",
    "        if h + 1 < th or w + 1 < tw:\n",
    "            raise ValueError(\n",
    "                \"Required crop size {} is larger then input image size {}\".format((th, tw), (h, w))\n",
    "            )\n",
    "\n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "\n",
    "        i = torch.randint(0, h - th + 1, size=(1, )).item()\n",
    "        j = torch.randint(0, w - tw + 1, size=(1, )).item()\n",
    "        return i, j, th, tw\n",
    "\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, images):\n",
    "        lq, gt = images['lq'], images['gt']\n",
    "\n",
    "        width, height = TVF._get_image_size(gt)\n",
    "\n",
    "        i, j, h, w = self.get_params(gt, self.size)\n",
    "\n",
    "        return {'lq':TVF.crop(lq, i, j, h, w), 'gt':TVF.crop(gt, i, j, h, w)}\n",
    "\n",
    "class CustomRandomHorizontalFlip():\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, images):\n",
    "        lq, gt = images['lq'], images['gt']\n",
    "        if torch.rand(1) < self.p:\n",
    "            return {'lq': TVF.hflip(lq), 'gt': TVF.hflip(gt)}\n",
    "        return images\n",
    "\n",
    "class CustomRandomRotation():\n",
    "    def __init__(self, degrees=90, p=0.5):\n",
    "        self.degrees = degrees\n",
    "        self.p = p\n",
    "  \n",
    "    def __call__(self, images):\n",
    "        lq, gt = images['lq'], images['gt']\n",
    "        if torch.rand(1) < self.p:\n",
    "            return {'lq': TVF.rotate(lq, angle=self.degrees), 'gt':TVF.rotate(gt, angle=self.degrees)}\n",
    "        return images\n",
    "\n",
    "class CustomToTensor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, images):\n",
    "        lq, gt = images['lq'], images['gt']\n",
    "        return {'lq': TVF.to_tensor(lq), 'gt': TVF.to_tensor(gt)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoPro(Dataset):\n",
    "    def __init__(self, root='./dataset', mode='train', new_datasets=False):\n",
    "        super(GoPro, self).__init__()\n",
    "        # 1. 이미지들의 경로 저장\n",
    "        # 2. 이미지 전처리 옵션 설정\n",
    "        if new_datasets:\n",
    "            if mode == 'train':\n",
    "                self.lqs = sorted(glob(f'{root}/new_{mode}/blur_crops/*'))\n",
    "                self.gts = sorted(glob(f'{root}/new_{mode}/sharp_crops/*'))\n",
    "            elif mode == 'test':\n",
    "                self.lqs = sorted(glob(f'./testset/*'))\n",
    "                self.gts = sorted(glob(f'./testset/*'))\n",
    "        else:\n",
    "            self.lqs = sorted(glob(f'{root}/{mode}/*/blur/*'))\n",
    "            self.gts = sorted(glob(f'{root}/{mode}/*/sharp/*'))\n",
    "            \n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "\n",
    "        self.transform_train = transforms.Compose([\n",
    "            # CustomToTensor(),\n",
    "            CustomRandomHorizontalFlip(p=0.5), # 다양한 이미지를 추출하기 위해 적용\n",
    "            CustomRandomRotation(degrees=90),    # 256 정방형으로 크기를 고정하고 회전시키는게 좋을거같다는 판단ㄴ\n",
    "            CustomRandomCrop((256, 256)), # @@@ Resize는 사진을 변형시켜서, 나중에 256x256으로 resize한 사진에만 훈련효과를 볼수있음\n",
    "        ])\n",
    "\n",
    "        self.transform_test = transforms.Compose([\n",
    "            # transforms.CenterCrop(224),\n",
    "            # CustomRandomCrop((256, 256)), # @@@ Resize는 사진을 변형시켜서, 나중에 256x256으로 resize한 사진에만 훈련효과를 볼수있음\n",
    "        ])\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        lq = transforms.ToTensor()(Image.open(self.lqs[index]).convert('RGB'))\n",
    "        gt = transforms.ToTensor()(Image.open(self.gts[index]).convert('RGB'))\n",
    "        \n",
    "        if self.mode == 'train': images = self.transform_train({'lq':lq, 'gt':gt})\n",
    "        elif self.mode == 'test': images = self.transform_test({'lq':lq, 'gt':gt})\n",
    "            \n",
    "        return images['lq'], images['gt']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSNRLoss(nn.Module):\n",
    "    def __init__(self, loss_weight=1.0, reduction='mean'):\n",
    "        super(PSNRLoss, self).__init__()\n",
    "        assert reduction == 'mean'\n",
    "        self.loss_weight = loss_weight\n",
    "        self.scale = 10 / np.log(10)\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        return self.loss_weight * self.scale * torch.log(((pred - target) ** 2).mean(dim=(1, 2, 3)) + 1e-8).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFTLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        diff = torch.fft.fft2(input) - torch.fft.fft2(target)\n",
    "        loss = torch.mean(abs(diff))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "batch_size = 4\n",
    "test_batch_size = 1\n",
    "shuffle = True\n",
    "\n",
    "# Network\n",
    "img_channel = 3\n",
    "width = 32\n",
    "enc_blks = [1, 1, 1, 13]\n",
    "middle_blk_num = 1\n",
    "dec_blks = [1, 1, 1, 1]\n",
    "\n",
    "# Loss\n",
    "losses = ['L1Loss', 'PSNRLoss', 'FFTLoss']\n",
    "loss = 'FFTLoss_DO'\n",
    "loss_folders = [f'd:/checkpoint/{loss}', f'd:/img/result/{loss}']\n",
    "for loss_folder in loss_folders:\n",
    "    if not os.path.exists(loss_folder): os.makedirs(loss_folder)\n",
    "\n",
    "# Train\n",
    "resume_train = True\n",
    "try: curr_epoch = list(map(lambda x : x[-8:-4], glob(f'd:/checkpoint/{loss}/*')))[-1]\n",
    "except: curr_epoch = '0000'; resume_train = False\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloading\n",
    "train_ds = GoPro(root='./dataset', mode='train', new_datasets=True)\n",
    "test_ds = GoPro(root='./dataset', mode='test', new_datasets=True)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle, num_workers=0, pin_memory=True, drop_last=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=test_batch_size, shuffle=False, num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "# Network\n",
    "net = NAFNet(img_channel=img_channel, width=width, middle_blk_num=middle_blk_num,\n",
    "                    enc_blk_nums=enc_blks, dec_blk_nums=dec_blks)\n",
    "net.to(device)\n",
    "\n",
    "# Loss\n",
    "if loss == 'L1Loss':\n",
    "    cri_pix = nn.L1Loss()\n",
    "elif loss == 'PSNRLoss':\n",
    "    cri_pix = PSNRLoss()\n",
    "elif loss == 'FFTLoss':\n",
    "    # cri_pix = PSNRLoss()\n",
    "    cri_pix = nn.L1Loss()\n",
    "    cri_pix_fft = FFTLoss()\n",
    "    cri_pix_fft.to(device)\n",
    "else:\n",
    "    cri_pix = nn.L1Loss()\n",
    "\n",
    "cri_pix.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer_g = torch.optim.AdamW(net.parameters(), lr=1e-3, weight_decay=1e-3, betas=(0.9, 0.9))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_g, T_max=int(len(train_ds)/batch_size), eta_min=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset check\n",
    "# lq, gt = next(iter(train_dl))\n",
    "# img_sample = torch.cat((lq, gt), -2) # 높이(height)를 기준으로 이미지를 연결하기\n",
    "# save_image(img_sample, \"train_dl_img.png\", nrow=batch_size, normalize=False)\n",
    "\n",
    "# # # Network check\n",
    "# # from torchsummary import summary\n",
    "# # summary(net,(3,256,256),batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try: curr_epoch = list(map(lambda x : x[-8:-4], glob(f'd:/checkpoint/{loss}/*')))[-1]\n",
    "# except: curr_epoch = '0000'; resume_train = False\n",
    "\n",
    "# if resume_train:\n",
    "#     state_dict = torch.load(f'd:/checkpoint/{loss}/checkpoint.{curr_epoch}.pth')\n",
    "\n",
    "#     resume_epoch = state_dict['epoch'] + 1  # +1 부터 시작할 수 있게\n",
    "#     optimizer_state_dict = state_dict['optimizer_state_dict']\n",
    "#     model_state_dict = state_dict['model_state_dict']\n",
    "#     scheduler_state_dict = state_dict['scheduler_state_dict']\n",
    "\n",
    "#     print(f'resume_epoch: {resume_epoch}')\n",
    "\n",
    "#     optimizer_g.load_state_dict(optimizer_state_dict)\n",
    "#     net.load_state_dict(model_state_dict, strict=True)\n",
    "#     scheduler.load_state_dict(scheduler_state_dict)\n",
    "# else:\n",
    "#     resume_epoch = 0\n",
    "\n",
    "# for epoch in range(resume_epoch, epochs):\n",
    "#     batch_cnt = 0\n",
    "#     pbar = tqdm(train_dl)\n",
    "#     for i, batch in enumerate(pbar):\n",
    "#         batch_cnt += 1\n",
    "\n",
    "#         lq, gt = batch\n",
    "#         lq = lq.to(device)\n",
    "#         gt = gt.to(device)\n",
    "        \n",
    "#         optimizer_g.zero_grad()\n",
    "\n",
    "#         preds_t = net(lq)\n",
    "#         if not isinstance(preds_t, list): # preds가 tensor라서 이걸 list로 바꿔주는거\n",
    "#             preds = [preds_t]\n",
    "\n",
    "#         output = preds[-1]\n",
    "\n",
    "#         # pixel loss\n",
    "#         l_total = 0.\n",
    "#         l_pix = 0.\n",
    "#         l_pix_fft = 0.\n",
    "#         for pred in preds:\n",
    "#             l_pix += cri_pix(pred, gt)   # 누적연산은 좋지않댔는데, float를 붙이면 해결가능함 (https://pytorch.org/docs/stable/notes/faq.html#my-model-reports-cuda-runtime-error-2-out-of-memory)\n",
    "#             #----------------------------------------------------------#\n",
    "#             l_pix_fft += cri_pix_fft(pred, gt)\n",
    "#             #----------------------------------------------------------#\n",
    "        \n",
    "#         l_total += ((0.05 * l_pix_fft) + l_pix)\n",
    "\n",
    "#         l_total = l_total + 0. * sum(p.sum() for p in net.parameters())\n",
    "#         pbar.set_postfix_str(f\"epoch: {epoch}, l_pix: {l_pix}, l_pix_fft: {0.05 * l_pix_fft}, l_total: {l_total} lr: {optimizer_g.param_groups[0]['lr']}\")\n",
    "\n",
    "\n",
    "#         l_total.backward()\n",
    "#         use_grad_clip = True\n",
    "#         if use_grad_clip:\n",
    "#             torch.nn.utils.clip_grad_norm_(net.parameters(), 0.01)\n",
    "#         optimizer_g.step()\n",
    "\n",
    "#         if batch_cnt % 300 == 0:\n",
    "#             img_sample = torch.cat((lq.data, preds_t, gt.data), -2) # 높이(height)를 기준으로 이미지를 연결하기\n",
    "#             save_image(img_sample, f\"d:/img/result/{loss}/result{epoch:04d}_{batch_cnt}.png\", nrow=batch_size, normalize=False)    \n",
    "    \n",
    "#     scheduler.step()\n",
    "\n",
    "#     img_sample = torch.cat((lq.data, preds_t, gt.data), -2) # 높이(height)를 기준으로 이미지를 연결하기\n",
    "#     save_image(img_sample, f\"d:/img/result/{loss}/result{epoch:04d}.png\", nrow=batch_size, normalize=False)\n",
    "#     print(f\"[Sample img 저장 완료] d:/img/result/{loss}/result{epoch:04d}.png\")\n",
    "    \n",
    "#     if epoch % 3 == 0:\n",
    "#         torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'optimizer_state_dict': optimizer_g.state_dict(),\n",
    "#                 'scheduler_state_dict':scheduler.state_dict(),\n",
    "#                 'model_state_dict': net.state_dict(),\n",
    "#             }, f'd:/checkpoint/{loss}/checkpoint.{epoch:04d}.pth')\n",
    "#         print(f'[Check point 저장 완료] d:/checkpoint/{loss}/checkpoint.{epoch:04d}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint0635를 이용하여 Inference 진행\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    isinstance(net, NAFNet)\n",
    "except:\n",
    "    net = NAFNet(img_channel=img_channel, width=width, middle_blk_num=middle_blk_num,\n",
    "                    enc_blk_nums=enc_blks, dec_blk_nums=dec_blks)\n",
    "    net.to(device)\n",
    "\n",
    "try: curr_epoch = list(map(lambda x : x[-8:-4], glob(f'd:/checkpoint/{loss}/*')))[-1]; \n",
    "except: raise Exception(f'd:/checkpoint/{loss}/ 에 체크포인트가 없습니다.')\n",
    "# curr_epoch = '0000'\n",
    "print(f'checkpoint{curr_epoch}를 이용하여 Inference 진행')\n",
    "state_dict = torch.load(f'd:/checkpoint/{loss}/checkpoint.{curr_epoch}.pth')\n",
    "model_state_dict = state_dict['model_state_dict']\n",
    "net.load_state_dict(model_state_dict, strict=True)  # 왜 넣어야하는거지?\n",
    "net.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample 이미지 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lq, gt = next(iter(test_dl))\n",
    "# lq = lq.to(device)\n",
    "# gt = gt.to(device)\n",
    "# img_res = net(lq)\n",
    "\n",
    "# img_sample = torch.cat((lq.data, img_res.data, gt.data), -2) # 높이(height)를 기준으로 이미지를 연결하기\n",
    "# save_image(img_sample, f\"img/result/{loss}/result{curr_epoch}_test.png\", nrow=batch_size, normalize=False)\n",
    "# print(f\"[저장 완료] img/result/{loss}/result{curr_epoch}_test.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSNR 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:13<00:00,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.psnr_measure import psnr\n",
    "import time\n",
    "psnr_avg = 0\n",
    "sum_psnr = 0\n",
    "curr_psnr = 0\n",
    "pbar = tqdm(test_dl)\n",
    "for i, batch in enumerate(pbar):\n",
    "    if i == 0: continue\n",
    "    # pbar.set_postfix_str(f\"current psnr_avg: {psnr_avg}\")\n",
    "    # pbar.set_postfix_str(f\"current psnr_avg: {psnr_avg}\")\n",
    "    \n",
    "    lq, gt = batch\n",
    "    lq = lq.to(device)\n",
    "    gt = gt.to(device)\n",
    "    results = net(lq)\n",
    "    for j, result in enumerate(results):\n",
    "        # curr_psnr = psnr(result, gt[j])\n",
    "        img_sample = torch.cat((result.data, gt[j].data), -2) # 높이(height)를 기준으로 이미지를 연결하기\n",
    "        save_image(img_sample, f\"tmp2/{test_batch_size*i+j}.png\", nrow=batch_size, normalize=False)\n",
    "    \n",
    "        # sum_psnr += curr_psnr\n",
    "        # psnr_avg = sum_psnr / (test_batch_size*i + j)\n",
    "        # print(curr_psnr)\n",
    "    # for result in results:\n",
    "        # sum_psnr += psnr(result, gt)\n",
    "    # psnr_avg = sum_psnr / (i * batch_size)\n",
    "\n",
    "# print(psnr_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference With CPU\n",
    "demo_image = transforms.ToTensor()(Image.open('./blurry.png').convert(\"RGB\")).unsqueeze(dim=0)\n",
    "net.to('cpu')\n",
    "img_res = net(demo_image)\n",
    "save_image(img_res, f\"blurry_result_{loss}_{curr_epoch}.png\", nrow=batch_size, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('yolo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff0323b92811708b19364feae987dc3a6297456b8196963308283135522dbebe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
